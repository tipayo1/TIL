<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" class="logo" width="120"/>

# llm에서 단어와 단어 이후에 어떤 단어가 나올지 예측하는걸 무엇이라 하는가

LLM에서 이전 단어들을 바탕으로 다음에 어떤 단어가 나올지 예측하는 것을 **언어 모델링(Language Modeling)** 또는 **다음 단어 예측(Next Word Prediction)**이라고 합니다[^1_1].

## 언어 모델링의 정의

언어 모델링은 주어진 단어들로부터 아직 모르는 단어를 예측하는 작업을 의미합니다[^1_1]. 즉, 언어 모델이 이전 단어들로부터 다음 단어를 예측하는 일이 바로 언어 모델링입니다[^1_1].

## 작동 원리

LLM은 **조건부 확률**을 기반으로 다음 단어를 예측하는 확률 기계로 작동합니다[^1_2]. 예를 들어, "나는 오늘 아침에 커피를 마셨다"라는 문장이 주어졌을 때, 모델은 이 문장 다음에 올 단어의 확률을 다음과 같이 계산합니다[^1_2]:

- "그리고"가 올 확률이 40%
- "하지만"이 올 확률이 20%
- "왜냐하면"이 올 확률이 10%

이러한 확률 분포를 통해 가장 적절한 단어를 선택하며, 이를 반복하여 문장을 생성해 나갑니다[^1_2].

## 수학적 표현

이 과정은 조건부 확률로 다음과 같이 표현됩니다[^1_3]:

**P(다음 단어 | 이전 단어들)**

n-1개의 단어가 나열된 상태에서 n번째 단어의 확률은 다음과 같습니다[^1_1]:

\$ P(w_n | w_1, ..., w_{n-1}) \$

## 실제 예시

"비행기를 타려고 공항에 갔는데 지각을 하는 바람에 비행기를 [?]"라는 문장에서 사람은 쉽게 "놓쳤다"라고 예상할 수 있습니다[^1_1]. 기계도 마찬가지로 앞에 어떤 단어들이 나왔는지 고려하여 후보가 될 수 있는 여러 단어들에 대해서 등장 확률을 추정하고 가장 높은 확률을 가진 단어를 선택합니다[^1_1].

이러한 언어 모델링 기술은 통계적인 방법을 사용하여 언어의 구조와 의미를 파악하며, 주어진 맥락에서 가장 가능성이 높은 다음 단어 또는 문장을 예측하는 핵심 기능입니다[^1_4].

<div style="text-align: center">⁂</div>

[^1_1]: https://wikidocs.net/21668

[^1_2]: https://sfixer.tistory.com/337

[^1_3]: https://velog.io/@wkfwktka/언어모델Language-Model

[^1_4]: https://www.gnict.org/blog/130/글/챗gpt-llm-관련-용어-및-개/

[^1_5]: https://login-data.tistory.com/13

[^1_6]: https://teon98.tistory.com/80

[^1_7]: https://blog.naver.com/bi_matrix/223381194556

[^1_8]: https://teon98.tistory.com/78

[^1_9]: https://www.oracle.com/kr/artificial-intelligence/large-language-model/

[^1_10]: https://brunch.co.kr/@@oZ3/1984


---

# nwp

**NWP**는 **Next Word Prediction(다음 단어 예측)**의 줄임말로, 자연어 처리(NLP)와 대규모 언어 모델(LLM)에서 핵심적인 개념입니다[^2_1][^2_2].

## 정의와 개념

NWP는 주어진 텍스트 시퀀스에서 다음에 올 가장 가능성 높은 단어를 예측하는 작업입니다[^2_3]. 이는 사용자가 다음에 입력할 가능성이 가장 높은 단어를 제안하는 자연어 처리의 한 분야로, 키보드를 사용하여 텍스트를 입력하는 다양한 영역에서 적용될 수 있습니다[^2_3].

## 작동 원리

NWP는 대규모 텍스트 파일(코퍼스) 분석을 기반으로 하여 코퍼스에서 발생하는 다양한 단어 시퀀스에 대한 확률 분포를 생성합니다[^2_3]. 결과적으로 생성된 언어 모델은 가장 가능성 높은 다음 단어를 예측하는 데 사용됩니다[^2_3].

## 실제 예시

다음과 같은 단계별 예측 과정을 보여줍니다[^2_2]:


| 단계 | 주어진 텍스트 | 예측 단어 |
| :-- | :-- | :-- |
| 1 | [BOS] | Never |
| 2 | [BOS] Never | gonna |
| 3 | [BOS] Never gonna | give |
| 4 | [BOS] Never gonna give | you |
| 5 | [BOS] Never gonna give you | up |

## 활용 분야

NWP는 다양한 실용적 응용 분야에서 사용됩니다[^2_3]:

- **스마트 키보드**: SwiftKey와 같은 모바일 키보드에서 타이핑 경험 개선
- **자동완성 기능**: 텍스트 입력 효율성 향상
- **챗봇 및 텍스트 생성기**: 자연스러운 대화 흐름 생성
- **문해력 향상**: 작문 품질과 양의 개선, 철자 교정 지원


## 기술적 구현

NWP는 주로 다음과 같은 딥러닝 기술을 활용합니다[^2_4]:

- **순환 신경망(RNN)**과 **장단기 메모리(LSTM)** 네트워크
- **가변 길이 입력 시퀀스** 지원
- **대규모 텍스트 데이터셋**을 통한 학습으로 다양한 언어 패턴 포착


## 최신 연구 동향

최근 연구에서는 전통적인 NWP의 한계를 극복하기 위한 새로운 접근법들이 제안되고 있습니다. 예를 들어, BOW(Bottlenecked Next Word Exploration)와 같은 강화학습 프레임워크는 추론 병목을 도입하여 모델이 다음 토큰을 직접 예측하는 대신 먼저 추론 경로를 생성하도록 하여 더 견고한 추론 능력을 개발하고자 합니다[^2_1][^2_5].

NWP는 현대 언어 모델의 핵심 메커니즘으로, 텍스트 생성과 언어 이해 작업에서 중요한 역할을 담당하고 있습니다.

<div style="text-align: center">⁂</div>

[^2_1]: https://www.themoonlight.io/ko/review/bow-bottlenecked-next-word-exploration

[^2_2]: https://user.phil-fak.uni-duesseldorf.de/~samih/wp-content/uploads/2022/03/Language-Modeling-Vocabulary-Tokenization-1.pdf

[^2_3]: https://github.com/duf59/NWP_Shiny

[^2_4]: https://github.com/Arbazkhan-cs/Next-Word-Prediction

[^2_5]: https://arxiv.org/pdf/2506.13502.pdf

[^2_6]: https://velog.io/@hellemon/Nonparametric-Masked-Language-Modeling-논문-읽기

[^2_7]: https://arxiv.org/html/2212.00596v2

[^2_8]: https://research.google/blog/advances-in-private-training-for-production-on-device-language-models/

[^2_9]: https://www.abbreviations.com/NWP

[^2_10]: https://www.turing.com/kb/guide-on-word-embeddings-in-nlp


---

# gpt같은 모델을 쓸 때 토큰당 사용가격이 매겨지는데 입력당 토큰인지 출력당 토큰인지 모르겠어

GPT와 같은 모델에서는 **입력 토큰과 출력 토큰 모두에 대해 별도로 요금이 부과**됩니다. 즉, 사용자가 보내는 프롬프트(입력)와 모델이 생성하는 응답(출력) 모두에 대해 토큰당 비용을 지불해야 합니다.

## 입력 토큰 vs 출력 토큰 요금 구조

OpenAI의 요금 체계는 입력과 출력을 구분하여 다른 가격을 적용합니다[^3_1][^3_2]. 일반적으로 **출력 토큰이 입력 토큰보다 더 비쌉니다**. 예를 들어:


| 모델 | 입력 토큰 (100만개당) | 출력 토큰 (100만개당) |
| :-- | :-- | :-- |
| GPT-4 | \$30 | \$60 |
| GPT-3.5 Turbo | \$0.50 | \$1.50 |
| GPT-4 Turbo | \$10 | \$30 |

## 토큰 계산 방식

API 호출 시 다음과 같이 토큰이 계산됩니다[^3_3][^3_4]:

- **입력 토큰**: 사용자가 보내는 모든 텍스트 (프롬프트, 시스템 메시지, 대화 기록 포함)
- **출력 토큰**: 모델이 생성하는 응답 텍스트
- **추가 토큰**: 메시지당 약 4개의 토큰이 시스템 레벨에서 추가됨


## 실제 사용 예시

만약 9,000토큰의 컨텍스트, 1,000토큰의 프롬프트를 보내고 1,000토큰의 응답을 받는다면[^3_3]:

- **총 입력 토큰**: 10,000개 (9,000 + 1,000)
- **총 출력 토큰**: 1,000개
- **총 청구 토큰**: 11,000개 (입력과 출력 각각 다른 요금으로 계산)


## 왜 출력 토큰이 더 비싼가?

출력 토큰이 더 비싼 이유는 계산 비용 때문입니다[^3_5]. 모델이 텍스트를 생성할 때는 각 출력 토큰마다 전체 디코더를 통한 순방향 패스를 수행해야 하므로, 입력 토큰보다 훨씬 많은 연산이 필요합니다.

따라서 비용을 절약하려면 **프롬프트를 간결하게 작성하고, 필요한 만큼만 응답을 생성**하도록 하는 것이 중요합니다.

<div style="text-align: center">⁂</div>

[^3_1]: https://openai.com/api/pricing/

[^3_2]: https://milvus.io/ai-quick-reference/what-is-the-pricing-model-for-openai

[^3_3]: https://www.reddit.com/r/OpenAI/comments/13yuvzg/question_about_gpt_pricing_per_token/

[^3_4]: https://scalebytech.com/openai-api-pricing/

[^3_5]: https://community.openai.com/t/why-does-pricing-vary-by-input-tokens-instead-of-only-output-tokens/21833

[^3_6]: https://www.reddit.com/r/OpenAI/comments/1izpgct/gpt45_has_an_api_price_of_751m_input_and_1501m/

[^3_7]: https://platform.openai.com/docs/pricing

[^3_8]: https://dida.do/openai-s-api-pricing-cost-breakdown-for-gpt-3-5-gpt-4-and-gpt-4o

[^3_9]: https://community.openai.com/t/can-someone-explain-me-the-pricing-model/29491

[^3_10]: https://www.engineering.com/what-are-input-and-output-tokens-in-ai/


---

# NWP는 대규모 텍스트 파일(코퍼스) 분석을 기반으로 하여 코퍼스에서 발생하는 다양한 단어 시퀀스에 대한 확률 분포를 생성합니다. 결과적으로 생성된 언어 모델은 가장 가능성 높은 다음 단어를 예측하는 데 사용됩니다


---
에서 코퍼스가 뭔가

**코퍼스(Corpus)**는 자연언어 연구를 위해 특정한 목적을 가지고 언어의 표본을 추출한 집합으로, 한국어로는 **말뭉치**라고 번역합니다[^4_1]. 평소 우리가 쓰는 말이나 글을 컴퓨터가 읽을 수 있는 형태로 모아 놓은 언어 자료입니다[^4_2].

## 코퍼스의 정의와 특징

코퍼스는 글 또는 말 텍스트를 모아 놓은 것으로, 언어학 분야에서 특정 주제에 맞춰 낱말들을 모아 놓은 '언어 자료'라는 의미를 지닙니다[^4_3]. 컴퓨터에 저장된 자연어 자료 모음이며 언어가 어떻게 사용됐는지 알아내는 데에 사용됩니다[^4_4].

둘 이상의 코퍼스가 존재하면 **코포라(Corpora)**라고 부릅니다[^4_5][^4_6].

## 코퍼스의 구성과 구축 과정

코퍼스를 구축하려면 신문 기사, 책, 일상 대화, 메신저 대화, 블로그나 게시판의 글 등의 언어 자료를 모아서 원저작자의 저작권 이용 허락 동의를 받은 후 컴퓨터가 읽을 수 있는 형태로 입력합니다[^4_2]. 이 언어 자료의 종류나 제목, 작성자, 출처 등 언어 자료의 특징이나 성격을 알려 주는 정보와 문단이나 문장의 경계를 알려 주는 형식 정보 등을 컴퓨터가 읽을 수 있도록 입력하면 말뭉치의 기본이라고 할 수 있는 원시 말뭉치가 됩니다[^4_2].

## 코퍼스의 활용 목적

코퍼스를 통해 자연어에 대한 빈도 분포, 단어의 동시 발생 등과 같은 통계적인 분석이 가능합니다[^4_5]. 다양한 자연어처리 과정에서 자연어 데이터에 대한 언어 규칙을 정의하고 검증할 수 있으며, 규칙 기반 시스템의 도움으로 언어에 따라 각 언어에 대한 언어 규칙을 정의할 수 있게 합니다[^4_5].

## 코퍼스의 종류

구성된 언어를 기준으로 다음과 같이 분류됩니다[^4_5][^4_6]:

- **단일 언어 코퍼스**: 하나의 언어로 이루어짐
- **이중 언어 코퍼스**: 2개의 언어로 이루어짐 (예: 한글, 영어)
- **다국어 코퍼스**: 3개 이상의 언어로 이루어짐 (번역)


## NLP에서의 역할

자연어 처리 분야에서 말뭉치는 언어 모델을 훈련하고, 언어의 통계적 패턴을 분석하며, 다양한 컴퓨터 기반 언어 연구에 사용됩니다[^4_3]. 자연어 처리에서 말뭉치의 질과 크기는 모델의 성능에 큰 영향을 미치며, 대규모의 다양한 말뭉치는 모델이 언어의 다양한 측면을 학습하는 데 도움을 줍니다[^4_3].

따라서 NWP에서 언급된 코퍼스는 언어 모델이 단어 시퀀스의 확률 분포를 학습하기 위한 기본 데이터셋 역할을 하는 대규모 텍스트 모음집이라고 할 수 있습니다.

<div style="text-align: center">⁂</div>

[^4_1]: https://ko.wikipedia.org/wiki/말뭉치

[^4_2]: https://kli.korean.go.kr/introduce/introduceList.do

[^4_3]: https://wikidocs.net/179495

[^4_4]: https://tekhartha.com/nlp/2018/11/29/NLP\&Corpus/

[^4_5]: https://pasongsong.tistory.com/398

[^4_6]: https://velog.io/@ssook1222/NLP-코퍼스와-데이터세트의-실제-이해

[^4_7]: http://www.aistudy.com/linguistics/corpus.htm

[^4_8]: https://transwriting.tistory.com/20

[^4_9]: https://encykorea.aks.ac.kr/Article/E0079634

[^4_10]: https://stdict.korean.go.kr/search/searchView.do?word_no=517854\&searchKeywordTo=3


---

# 코퍼스와 토큰의 차이

코퍼스와 토큰은 자연어 처리에서 서로 다른 개념으로, 데이터의 규모와 처리 단계에서 차이가 있습니다.

## 코퍼스(Corpus)

코퍼스는 **대규모 언어 데이터의 집합체**입니다[^5_1][^5_2]. 자연언어 연구를 위해 특정한 목적을 가지고 언어의 표본을 추출한 집합으로, 한국어로는 **말뭉치**라고 번역합니다[^5_3]. 컴퓨터에 저장된 자연어 자료 모음이며 언어가 어떻게 사용됐는지 알아내는 데에 사용됩니다[^5_4].

코퍼스의 특징:

- **대용량 컴퓨터에 저장된 인간의 음성언어**(문어, 구어) 데이터베이스[^5_2]
- **컴퓨터가 판독할 수 있는 형태**로 저장된 자연어 용례들과 부속정보[^5_2]
- 신문 기사, 책, 일상 대화, 블로그 글 등 다양한 언어 자료를 포함[^5_2]


## 토큰(Token)

토큰은 **코퍼스를 의미있는 최소 단위로 나눈 개별 요소**입니다[^5_1][^5_3]. 주어진 코퍼스에서 토큰이라 불리는 단위로 나누는 작업을 토큰화(tokenization)라고 합니다[^5_1]. 토큰의 단위가 상황에 따라 다르지만, 보통 의미있는 단위로 토큰을 정의합니다[^5_3].

토큰의 특징:

- **단어, 문장, 문자 등의 개별 단위**로 구분됨[^5_3]
- 예: "This is a pen" → "this", "is", "a", "pen"으로 토큰화[^5_3]
- 한국어: "아버지가 방에 들어가신다" → "아버지", "가", "방", "에", "들어가", "신다"[^5_3]


## 주요 차이점

| 구분 | 코퍼스 | 토큰 |
| :-- | :-- | :-- |
| **규모** | 대규모 데이터 집합 | 개별 최소 단위 |
| **역할** | 언어 연구의 원재료 | 처리 가능한 개별 요소 |
| **처리 단계** | 원시 데이터 상태 | 전처리된 분석 단위 |
| **예시** | 전체 신문 기사 모음 | "안녕", "하세요" 등 개별 단어 |

## 관계성

코퍼스와 토큰은 **전체와 부분의 관계**입니다. 코퍼스는 언어 모델 학습을 위한 **원재료 데이터베이스** 역할을 하며, 이 코퍼스를 토큰화 과정을 통해 **개별 처리 단위인 토큰들로 분해**합니다[^5_1].

자연어 처리에서는 먼저 대규모 코퍼스를 수집하고, 이를 토큰화하여 모델이 학습할 수 있는 형태로 변환하는 과정을 거칩니다. 따라서 코퍼스는 **데이터의 저장소**, 토큰은 **실제 처리되는 분석 단위**라고 할 수 있습니다.

<div style="text-align: center">⁂</div>

[^5_1]: https://wikidocs.net/21698

[^5_2]: https://transwriting.tistory.com/20

[^5_3]: https://yuls-with-ai.tistory.com/235

[^5_4]: https://tekhartha.com/nlp/2018/11/29/NLP\&Corpus/

[^5_5]: https://cryptomus.com/ko/blog/what-is-a-token-in-cryptocurrency

[^5_6]: https://didu-story.tistory.com/70

[^5_7]: https://velog.io/@sobit/자연어-처리natural-language-processing-NLP-기본-개념

[^5_8]: https://undeadkwandoll.tistory.com/33

[^5_9]: https://coding-yesung.tistory.com/227

[^5_10]: https://blog.naver.com/psu0903/221448137039


---

# 코퍼스와 단어와 토큰과 벡터의 차이

코퍼스, 단어, 토큰, 벡터는 자연어 처리에서 서로 다른 단계와 형태를 나타내는 개념들로, 데이터 처리 과정에서 순차적으로 변환되는 관계에 있습니다.

## 코퍼스(Corpus)

코퍼스는 **자연언어 연구를 위해 특정한 목적을 가지고 언어의 표본을 추출한 집합**입니다[^6_1]. 한국어로는 말뭉치라고 번역하며, **대규모 언어 데이터베이스**로서 인간의 음성언어(문어, 구어)를 대용량 컴퓨터에 저장한 것입니다[^6_2]. 신문 기사, 책, 일상 대화, 블로그 글 등 다양한 언어 자료를 포함하는 원시 데이터 집합체입니다.

## 단어(Word)

단어는 **의미를 갖는 언어의 기본 단위**로, 일반적으로 띄어쓰기로 구분되는 개별 언어 요소입니다. 예를 들어 "자연어 처리는 흥미롭다"라는 문장에서 "자연어", "처리는", "흥미롭다"가 각각 단어에 해당합니다. 단어는 아직 컴퓨터가 직접 처리할 수 없는 자연어 형태입니다.

## 토큰(Token)

토큰은 **주어진 코퍼스에서 의미있는 단위로 나눈 개별 요소**입니다[^6_1]. **토큰화(tokenization)**는 코퍼스를 토큰이라 불리는 단위로 나누는 작업으로, 보통 의미있는 단위로 토큰을 정의합니다[^6_3][^6_1].

예시:

- "This is a pen" → "this", "is", "a", "pen"으로 토큰화[^6_1]
- "아버지가 방에 들어가신다" → "아버지", "가", "방", "에", "들어가", "신다"로 토큰화[^6_1]

토큰화에는 **단어 토큰화(Word Tokenization)**와 **문장 토큰화(Sentence Tokenization)**가 있습니다[^6_1].

## 벡터(Vector)

벡터는 **토큰을 컴퓨터가 처리할 수 있는 수치 형태로 변환한 것**입니다[^6_4]. 자연어 처리에서 **임베딩(Embedding)**은 단어나 문장을 벡터로 변환하는 기술로, 단어나 문장을 벡터로 변환하면 컴퓨터가 이해하기 쉽게 됩니다[^6_4].

벡터화 방법:

- **원핫 벡터(One-hot vector)**: 단어를 숫자로 인코딩하는 간단한 방법[^6_4]
- **Word2vec**: 단어의 의미와 문맥 정보를 파악하여 벡터 공간에 매핑[^6_4]


## 처리 과정과 관계

| 단계 | 형태 | 설명 | 예시 |
| :-- | :-- | :-- | :-- |
| **코퍼스** | 원시 데이터 | 대규모 텍스트 집합 | 전체 신문 기사 모음 |
| **단어** | 자연어 단위 | 의미를 갖는 언어 요소 | "자연어", "처리" |
| **토큰** | 분석 단위 | 처리 가능한 개별 요소 | "자연어", "처리", "는" |
| **벡터** | 수치 표현 | 컴퓨터 처리 가능한 형태 | [0.2, 0.8, -0.1, ...] |

## 변환 과정

자연어 처리에서는 다음과 같은 순서로 데이터가 변환됩니다:

**코퍼스 수집** → **토큰화** → **벡터화** → **모델 학습**

1. **코퍼스**: 언어 연구의 원재료 데이터베이스
2. **토큰**: 코퍼스를 의미 있는 단위로 분해한 개별 요소
3. **벡터**: 토큰을 수치화하여 기계학습에 활용할 수 있는 형태

이러한 변환 과정을 통해 자연어가 컴퓨터가 이해하고 처리할 수 있는 형태로 변환되어 다양한 자연어 처리 작업에 활용됩니다.

<div style="text-align: center">⁂</div>

[^6_1]: https://yuls-with-ai.tistory.com/235

[^6_2]: https://transwriting.tistory.com/20

[^6_3]: https://wikidocs.net/21698

[^6_4]: https://pkait.tistory.com/9

[^6_5]: https://velog.io/@boyunj0226/토큰화-알고리즘-비교-정리

[^6_6]: https://dsc-sookmyung.tistory.com/294

[^6_7]: https://changsroad.tistory.com/415

[^6_8]: https://welcome-to-dewy-world.tistory.com/97

[^6_9]: https://codingopera.tistory.com/35

[^6_10]: https://velog.io/@lighthouse97/텍스트-전처리-토큰화


---

# 코퍼스가 자연어 이해에 어떤 도움을 주는지 알고 싶어요

코퍼스는 자연어 이해에 다양한 측면에서 핵심적인 도움을 제공합니다. 대규모 언어 데이터 집합인 코퍼스를 통해 컴퓨터가 인간의 언어를 이해하고 처리할 수 있는 기반을 마련할 수 있습니다.

## 통계적 언어 분석 기반 제공

코퍼스의 도움으로 자연어에 대한 **빈도 분포, 단어의 동시 발생** 등과 같은 통계적인 분석이 가능합니다[^7_5]. 이러한 통계적 정보는 언어의 패턴을 파악하고 단어 간의 관계를 이해하는 데 필수적입니다. 예를 들어, 특정 단어가 다른 단어와 함께 나타나는 빈도를 분석하여 의미적 연관성을 파악할 수 있습니다.

## 언어 규칙 정의와 검증

다양한 자연어처리 과정에서 자연어 데이터에 대한 **언어 규칙을 정의하고 검증**할 수 있으며, 규칙 기반 시스템의 도움으로 언어에 따라 각 언어에 대한 언어 규칙을 정의할 수 있게 합니다[^7_5]. 코퍼스를 통해 실제 언어 사용 패턴을 관찰하고, 이를 바탕으로 문법적, 의미적 규칙을 수립할 수 있습니다[^7_2].

## 모델 성능 향상

자연어 처리에서 **말뭉치의 질과 크기는 모델의 성능에 큰 영향을 미칩니다**. 대규모의 다양한 말뭉치는 모델이 언어의 다양한 측면을 학습하는 데 도움을 주지만, 편향된 말뭉치는 모델의 일반화 능력을 제한할 수 있습니다[^7_1].

**코퍼스가 많고 오류가 적을수록 NLP 모델은 더 정교해지고 정확도가 높아지게 됩니다**[^7_3]. 이는 모델이 더 많은 언어 패턴과 변형을 학습할 수 있기 때문입니다.

## 언어 모델 훈련의 핵심 자료

코퍼스는 **언어 모델을 훈련하고, 언어의 통계적 패턴을 분석하며, 다양한 컴퓨터 기반 언어 연구에 사용**됩니다[^7_1]. 대규모 언어 모델들은 방대한 코퍼스를 학습하여 다음 단어 예측, 문맥 이해, 의미 파악 등의 능력을 개발합니다.

## 다양한 언어 현상 학습

코퍼스는 특정 언어, 방언, 주제, 스타일 또는 시대를 대표하는 텍스트들을 포함할 수 있어[^7_1], 모델이 다양한 언어 사용 맥락을 이해할 수 있도록 돕습니다. 이를 통해:

- **문체와 장르별 특성** 학습
- **시대적 언어 변화** 파악
- **전문 분야별 용어** 이해
- **구어와 문어의 차이** 인식


## 실제 언어 사용 패턴 반영

코퍼스는 **진정한 의사 소통 상황의 맥락을 토대로 해서 언어 개념의 심층적인 조사를 수행하는 방법론**을 제공합니다[^7_2]. 이를 통해 인위적이지 않은 실제 언어 사용 패턴을 학습할 수 있어, 더욱 자연스럽고 정확한 언어 이해가 가능해집니다.

## 편향 방지와 일반화 능력 향상

다양한 분야의 코퍼스를 수집하는 것이 중요한 이유는 **데이터 편향**을 방지하기 위함입니다[^7_3]. 특정 분야에 치우치지 않은 균형잡힌 코퍼스는 모델의 일반화 능력을 향상시키고, 다양한 상황에서 안정적인 성능을 보장합니다.

결론적으로, 코퍼스는 자연어 이해를 위한 **기초 데이터이자 학습 자료**로서, 언어의 통계적 특성 파악, 규칙 정의, 모델 훈련, 성능 향상 등 자연어 처리의 모든 단계에서 핵심적인 역할을 담당합니다.

<div style="text-align: center">⁂</div>

[^7_1]: https://wikidocs.net/179495

[^7_2]: https://velog.io/@ssook1222/NLP-코퍼스와-데이터세트의-실제-이해

[^7_3]: https://jungnamgyu.tistory.com/60

[^7_4]: https://olivia-blackcherry.tistory.com/11

[^7_5]: http://contents2.kocw.or.kr/KOCW/data/document/2020/edu1/cuk/kimsungjoong1113/2.pdf

[^7_6]: https://blog.naver.com/irenekim516/221359291712

[^7_7]: https://yeon-code.tistory.com/53


---

# 코퍼스의 예

코퍼스는 다양한 목적과 언어에 따라 여러 종류가 구축되어 있으며, 각각 고유한 특성과 활용 방법을 가지고 있습니다.

## 영어 코퍼스

**Brown Corpus**는 세계 최초로 구축된 코퍼스로 코퍼스 설계의 전형이 되었습니다[^8_1][^8_2]. 1960년대에 만들어진 이 코퍼스는 영어 어휘 출현 빈도 등과 관련한 영어교육 및 심리학에서 코퍼스의 효용성을 입증했습니다[^8_2]. 미국 출판 비율을 고려한 장르별 텍스트 비율로 구성되었으며, 층화 표본추출과 단순 무작위적 표본추출 방법을 사용했습니다[^8_2].

**BNC(British National Corpus)**는 영국 표준 영어를 대상으로 하는 1억 어휘 규모의 코퍼스로, 문어 90%와 구어 10%로 구성되어 있으며 통계학적 분포와 사회 계층 분포를 고려하여 만들어졌습니다[^8_2].

**Bank of English**는 모니터 말뭉치의 예로, 늘 변화하는 언어의 실태를 추적하기 위하여 낡은 자료를 제외하고 늘 새로운 언어 정보를 수집, 증보하여 최신 언어 정보를 데이터베이스화한 것입니다[^8_3].

## 한국어 코퍼스

국립국어원에서 제공하는 **모두의 말뭉치**에는 다양한 종류의 한국어 코퍼스가 있습니다[^8_4]:

**일상 대화 관련 코퍼스**:

- **일상 대화 요약 말뭉치 2023**: 일상 대화를 대상으로 담화 분석하여 화자별 요약문, 주제별 요약문 및 대표 요약문을 작성한 말뭉치
- **대화 맥락 추론 말뭉치 2023**: 대화 맥락이나 상식, 세계 지식 등에 의거하여 5가지 유형별 추론문을 작성하여 구성한 말뭉치

**공식 문서 코퍼스**:

- **국회 회의록 요약 말뭉치 2023**: 국회 소위원회 회의록을 대상으로 쟁점별 중요 요약문, 세부 요약문 및 문서 전체에 대한 대표 요약문으로 구성한 말뭉치
- **신문 말뭉치 2023**: 2022년 생산된 신문 기사 중 매체로부터 저작권 이용 허락을 받은 기사를 기계 분석이 가능한 형식으로 정제한 말뭉치

**특수 목적 코퍼스**:

- **묵자-점자 병렬 말뭉치 2023**: 한국어 문어 데이터에서 한글, 로마자, 숫자, 기호의 조합 조건을 만족하는 문장을 추출하고 점역, 교정하여 구축한 병렬 말뭉치
- **한국어-한국수어 병렬 말뭉치 2022**: 한국어 구어 자료를 한국수어로 번역하여 구성한 병렬 말뭉치

**언어학 연구용 코퍼스**:

- **형태 분석 말뭉치**: 어절을 분석하여 형태 표지를 부착한 말뭉치
- **구문 분석 말뭉치**: 문장의 구문 구조를 분석해 의존 관계 표지를 부착한 말뭉치
- **개체명 분석 말뭉치**: 문장에 나타난 개체명의 경계를 표시하고 분석 표지를 부착한 말뭉치

**역사적 코퍼스**:

- **국어 역사 말뭉치**: 15세기 한글 창제 이후부터 20세기 초기까지 한글로 기록된 문헌자료 원시 말뭉치


## 다국어 코퍼스

**비교 코퍼스**는 구성 시기, 목표 청중, 텍스트 범주 등의 조건이 같은 두 언어의 코퍼스를 의미합니다[^8_5]. 예를 들어, 같은 시기에 비슷한 영향력을 가진 한중 뉴스 대본을 두 언어 간 보도 어휘를 비교하기 위한 코퍼스로 활용할 수 있습니다[^8_5].

**번역/병렬 코퍼스**는 한 언어의 원문과 원문을 번역한 여러 언어의 코퍼스를 의미합니다[^8_5]. 성경의 경우 원문 히브리어가 세계 여러 언어로 번역되었는데, 해당 어휘, 구, 표현 등이 다른 언어에서 어떻게 대응되어 번역되었는지 확인할 수 있습니다[^8_5].

## 코퍼스 분류

코퍼스는 다양한 기준으로 분류할 수 있습니다[^8_3]:

**텍스트 내용의 변화 여부에 따라**:

- **샘플 말뭉치**: 텍스트를 일정량만 수집한 것으로 텍스트의 내용이 고정되어 있음 (예: Brown Corpus, LOB Corpus)
- **모니터 말뭉치**: 늘 변화하는 언어의 실태를 추적하기 위하여 새로운 언어 정보를 지속적으로 수집하여 텍스트의 내용이 변화함 (예: Bank of English)

**연구 목적에 따라**:

- **범용말뭉치**: 여러 가지 연구에 폭넓게 이용할 수 있도록 종합적으로 작성된 말뭉치 (예: 21세기 세종계획 말뭉치)
- **특수목적 말뭉치**: 특정 언어 혹은 특수한 집단이나 장르의 언어를 연구하기 위하여 만들어진 말뭉치 (예: 의료 종사자 영어 말뭉치, 특정 연령층 언어 말뭉치)

이러한 다양한 코퍼스들은 각각의 특성에 맞는 연구 목적과 활용 방법을 가지고 있으며, 자연어 처리와 언어학 연구에서 중요한 역할을 담당하고 있습니다.

<div style="text-align: center">⁂</div>

[^8_1]: https://blog.naver.com/jyday/117582934

[^8_2]: https://data-est-her.tistory.com/8

[^8_3]: http://blog.naver.com/b4332/220263617191

[^8_4]: https://kli.korean.go.kr/corpus/main/requestMain.do

[^8_5]: https://euijuworld.tistory.com/56

[^8_6]: https://blog.naver.com/sijak-edu/222097342397

[^8_7]: https://datasciencebeehive.tistory.com/187

[^8_8]: http://contents2.kocw.or.kr/KOCW/data/document/2020/edu1/cuk/kimsungjoong1113/2.pdf

[^8_9]: https://normal-engineer.tistory.com/254

[^8_10]: https://www.youtube.com/watch?v=jF9C3lWimhQ


---

# 코퍼스의 예시

코퍼스는 다양한 목적과 언어에 따라 여러 종류가 구축되어 있으며, 각각 고유한 특성과 활용 방법을 가지고 있습니다.

## 영어 코퍼스

**Brown Corpus**는 세계 최초로 구축된 코퍼스로 코퍼스 설계의 전형이 되었습니다[^9_1]. 1960년대에 만들어진 이 코퍼스는 영어 어휘 출현 빈도 등과 관련한 영어교육 및 심리학에서 코퍼스의 효용성을 입증했습니다[^9_1]. 미국 출판 비율을 고려한 장르별 텍스트 비율로 구성되었으며, 층화 표본추출과 단순 무작위적 표본추출 방법을 사용했습니다[^9_1].

**BNC(British National Corpus)**는 영국 표준 영어를 대상으로 하는 1억 어휘 규모의 코퍼스로, 문어 90%와 구어 10%로 구성되어 있으며 통계학적 분포와 사회 계층 분포를 고려하여 만들어졌습니다[^9_1].

**Bank of English**는 모니터 말뭉치의 예로, 늘 변화하는 언어의 실태를 추적하기 위하여 낡은 자료를 제외하고 늘 새로운 언어 정보를 수집, 증보하여 최신 언어 정보를 데이터베이스화한 것입니다[^9_2].

**COCA(The Corpus of Contemporary American English)**는 미국영어 자료로, spoken, fiction, magazine, newspaper, academic 등 5개의 장르별로 구분하여 구성되어 있습니다[^9_3].

## 한국어 코퍼스

**21세기 세종계획 코퍼스**는 여러 가지 연구에 폭넓게 이용할 수 있도록 종합적으로 작성된 범용말뭉치의 대표적인 예입니다[^9_2].

**한국어 코퍼스 용어집**에서 제시하는 다양한 코퍼스 유형들:

- **범용/일반 코퍼스**: 한 언어의 다양한 사용역 자료를 적절한 비율로 구성하여 해당 언어의 대표성을 가질 수 있도록 구축한 것[^9_4]
- **원어민 코퍼스**: 언어 자료가 그 언어를 모국어로 하는 사람들의 것[^9_4]
- **웹 코퍼스**: 21세기에 접어들면서 인터넷상의 모든 웹 자료를 데이터베이스로 사용하는 새로운 개념의 코퍼스[^9_4]
- **주석 코퍼스**: 텍스트에 부가적인 정보를 덧붙여 컴퓨터로 유형 검색을 용이하게 할 수 있도록 만든 것[^9_4]


## 다국어 코퍼스

**비교 코퍼스**는 구성 시기, 목표 청중, 텍스트 범주 등의 조건이 같은 두 언어의 코퍼스를 의미합니다[^9_5]. 예를 들어, 같은 시기에 비슷한 영향력을 가진 한중 뉴스 대본을 두 언어 간 보도 어휘를 비교하기 위한 코퍼스로 활용할 수 있습니다[^9_5].

**번역/병렬 코퍼스**는 한 언어의 원문과 원문을 번역한 여러 언어의 코퍼스를 의미합니다[^9_5]. 성경의 경우 원문 히브리어가 세계 여러 언어로 번역되었는데, 해당 어휘, 구, 표현 등이 다른 언어에서 어떻게 대응되어 번역되었는지 확인할 수 있습니다[^9_5].

## 특수 목적 코퍼스

**의료 종사자 영어 말뭉치**는 의료 종사자가 사용하는 영어를 조사하기 위한 특수목적 말뭉치의 예입니다[^9_2]. **특정 연령층 언어 말뭉치**는 특정 연령층의 언어를 조사하기 위하여 만들어진 말뭉치입니다[^9_2].

## 시대별 분류

**Helsinki Corpus**는 750~1710년까지의 자료를 수록한 통시 말뭉치의 예이며, **ARCHER**는 1650~1990년 자료를 수록한 통시 말뭉치입니다[^9_2]. 반면 **Brown Corpus**와 **LOB Corpus**는 공시자료를 대상으로 한 공시 말뭉치의 예입니다[^9_2].

이러한 다양한 코퍼스들은 각각의 특성에 맞는 연구 목적과 활용 방법을 가지고 있으며, 자연어 처리와 언어학 연구에서 중요한 역할을 담당하고 있습니다.

<div style="text-align: center">⁂</div>

[^9_1]: https://data-est-her.tistory.com/8

[^9_2]: http://blog.naver.com/b4332/220263617191

[^9_3]: https://life-altering-challenges13.tistory.com/5

[^9_4]: http://icr.or.kr/corpus-glossaries_kr/

[^9_5]: https://euijuworld.tistory.com/56

[^9_6]: https://blog.naver.com/sijak-edu/222097342397

[^9_7]: http://contents2.kocw.or.kr/KOCW/data/document/2020/edu1/cuk/kimsungjoong1113/2.pdf

[^9_8]: http://corpus.korean.go.kr

[^9_9]: https://normal-engineer.tistory.com/254

[^9_10]: https://aihub.or.kr/aidata/7978

