<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" class="logo" width="120"/>

# OpenAI의 Whisper 모델의 언어 임베딩

OpenAI의 Whisper 모델은 다국어 음성 인식을 위한 혁신적인 접근 방식을 사용하며, 언어 임베딩과 관련된 독특한 메커니즘을 갖추고 있습니다.

## **언어 식별 및 토큰 기반 시스템**

Whisper는 **특별한 토큰 시스템**을 통해 언어를 처리합니다. 모델은 먼저 누군가 말하고 있는 언어를 감지하는 것부터 시작하며, **99개 각각의 언어마다 독특한 토큰을 사용**합니다[^1_5]. 이러한 언어별 토큰은 모델이 어떤 언어로 음성이 입력되었는지 식별하고, 해당 언어에 맞는 처리를 수행할 수 있도록 합니다.

## **멀티태스킹 아키텍처와 마커 토큰**

Whisper의 언어 처리는 **'마커(Markers)'라고 하는 특별한 토큰**을 통해 이루어집니다. 모델이 이러한 마커 토큰을 전달받아서 어떤 작업을 수행할지 지시를 받는 구조입니다[^1_5]. 이 시스템은 다음과 같이 작동합니다:

- **언어 감지**: 99개 언어 각각에 대한 고유 토큰 사용
- **작업 선택**: 음성을 전사할지, 번역할지 등을 결정하는 토큰
- **타임스탬프**: 필요시 각 단어의 시작 및 종료 시간을 예측
- **"음성 없음" 토큰**: 오디오에 음성이 없을 때 "<|nospeech|>" 토큰 예측


## **다국어 학습 데이터와 성능**

Whisper는 **680,000시간에 해당하는 레이블이 지정된 오디오 데이터**로 훈련되었으며, 그 중 **117,000시간(약 17%)은 非영어 오디오와 해당하는 非영어 언어의 전사본**입니다[^1_5]. 이러한 방대한 다국어 데이터셋을 통해 모델은 **총 99개 언어**를 지원할 수 있게 되었습니다[^1_5].

흥미롭게도, Whisper의 성능은 **언어별 학습 데이터의 양과 강한 상관관계**를 보입니다. 연구에 따르면 WER의 로그와 언어당 학습 데이터 양의 로그 사이에 0.83의 강한 제곱 상관 계수가 있으며, **학습 데이터가 16배 증가할 때마다 WER이 반으로 줄어드는** 패턴을 보입니다[^1_6].

## **언어적 특성과 성능 차이**

Whisper의 언어 처리 성능은 언어의 특성에 따라 차이를 보입니다. **고유한 스크립트를 가지고 있고 인도-유럽 언어와 관련이 먼 언어들**(히브리어, 텔루구어, 중국어, 한국어 등)은 예상보다 낮은 성능을 보이는 경우가 있습니다[^1_6]. 이는 언어적 거리로 인한 전송 부족, 바이트 레벨 BPE 토크나이저의 불일치, 또는 데이터 품질의 차이 때문일 수 있습니다.

## **Transformer 기반 아키텍처**

Whisper의 기초 모델은 **Transformer의 인코더-디코더 구조**를 따릅니다[^1_3]. 음성 데이터가 인코더에 입력되면, 해당 음성 데이터로 처리해야 하는 내용을 디코더에 입력하는 방식입니다. 인코더에 입력되는 음성 데이터는 **30초 단위로 분할**되며, 이후 **Mel-Spectrogram을 통해 변환**되어 처리됩니다[^1_3].

이러한 종합적인 언어 임베딩 시스템을 통해 Whisper는 **제로샷(Zero-Shot) 환경에서도 뛰어난 다국어 음성 인식 성능**을 보여주며, 추가적인 파인튜닝 없이도 전사와 번역 작업을 동시에 수행할 수 있는 강력한 모델로 자리잡았습니다[^1_4][^1_5].

<div style="text-align: center">⁂</div>

[^1_1]: https://jpjp9292.tistory.com/12

[^1_2]: https://tech.osci.kr/openai-whisper와-함께하는-음성-인식/

[^1_3]: https://stibee.com/api/v1.0/emails/share/BcufvAuC1vo7FYjMr5OQojuR6fPQpWo

[^1_4]: https://nlpcloud.com/ko/how-to-install-and-deploy-whisper-the-best-open-source-alternative-to-google-speech-to-text.html

[^1_5]: https://turingpost.co.kr/p/topic-15-openai-whisper

[^1_6]: https://kimjy99.github.io/논문리뷰/whisper/

[^1_7]: https://devwriter.tistory.com/102

[^1_8]: https://www.toolify.ai/ko/tool/whisper-github

[^1_9]: https://learn.microsoft.com/ko-kr/azure/ai-services/openai/concepts/models


---

# 위 논문의 내용을 요약해주

이 논문은 웨어러블 기기에서 공간 음성 처리를 위한 대규모 언어 모델 시스템인 **SING(Spatial Audio Processing with Large Language Model)**을 제안합니다[^2_1].

## **연구 배경 및 목적**

기존 대규모 언어 모델(LLM)은 텍스트 기반 상호작용에 뛰어나지만, 공간적 맥락을 이해하는 능력이 부족했습니다[^2_1]. 특히 웨어러블 기기에서 음성의 방향성과 공간 정보를 함께 처리할 수 있는 시스템의 필요성이 대두되었습니다[^2_1].

## **핵심 기술적 혁신**

**마이크로구조 기반 공간 센싱**: 전통적인 마이크로폰 어레이 대신 **단일 마이크로폰과 작은 마이크로구조를 결합**하여 공간 다양성을 구현했습니다[^2_1]. 이 시스템은 회절, 모세관 효과, 구조적 공명의 물리적 원리를 활용하여 음성의 도달 방향(DoA)에 따라 서로 다른 주파수 응답을 생성합니다[^2_1].

**멀티모달 임베딩 융합**: OpenAI의 Whisper 모델에서 추출한 언어적 임베딩과 공간 인코더에서 생성한 방향성 임베딩을 융합하여 **LLaMA-3.2 3B 모델의 입력 공간에 정렬**했습니다[^2_1]. 이를 위해 간단한 선형 레이어를 프로젝션 매트릭스로 사용하고, LoRA(Low-Rank Adaptation) 기법으로 경량화된 파인튜닝을 수행했습니다[^2_1].

## **데이터셋 구축**

LibriSpeech 데이터셋을 기반으로 **OmniTalk**이라는 합성 공간 음성 데이터셋을 생성했습니다[^2_1]. 이 데이터셋은 1°부터 360°까지 1° 해상도로 임펄스 응답을 적용하여 400시간의 공간 음성 인식용 데이터와 2,000시간의 다중 화자(1-5명) 사운드스케이핑용 데이터를 포함합니다[^2_1].

## **성능 결과**

**공간 인식 음성 인식**: DoA 추정에서 평균 절대 오차 **25.72°**를 달성하여 기존 BAT 모델의 88.52°보다 크게 개선되었습니다[^2_1]. 음성 인식의 경우 단어 오류율(WER) **5.3%**를 기록했으며, 공간 정보를 제외했을 때는 1.8%로 SALMONN의 2.2%보다 우수한 성능을 보였습니다[^2_1].

**다중 DoA 검출**: 최대 5명의 화자를 동시에 처리할 수 있으며, DoA 오차의 중앙값이 **16°**를 달성했습니다[^2_1]. 화자 수가 증가해도 안정적인 성능을 유지하여 1명일 때 25.72°, 5명일 때 17.08°의 평균 절대 오차를 보였습니다[^2_1].

## **응용 분야 및 의의**

이 시스템은 **증강 현실, 접근성 도구, 몰입형 경험** 등 다양한 웨어러블 애플리케이션에 활용될 수 있습니다[^2_1]. 특히 회의에서의 선택적 음성 요약, 시각 장애인을 위한 음성 기반 내비게이션, 사용자 주변 환경에 동적으로 반응하는 XR 경험 등이 가능합니다[^2_1].

이 연구는 **최소한의 하드웨어로 정확한 공간 음성 처리**를 가능하게 하여 웨어러블 기기의 전력 효율성, 프라이버시, 하드웨어 제약 문제를 해결하면서도 고성능을 달성한 혁신적인 접근법을 제시했습니다[^2_1].

<div style="text-align: center">⁂</div>

[^2_1]: SING-casedae-weeoreobeuleul-wihan-daegyumo-eoneo-modelyi-gongganjeog-maegrag.pdf


---

# 위에서 말하는 공간적 맥락이란

검색 결과를 바탕으로 **공간적 맥락(Spatial Context)**은 여러 분야에서 다양한 의미로 사용되고 있습니다.

## **웨어러블 기기에서의 공간적 맥락**

SING 시스템에서 공간적 맥락은 **음성의 방향성과 공간 정보를 포함하는 오디오 단서**를 의미합니다[^3_1][^3_2]. 구체적으로는 **방향성(directionality)과 잔향(reverberation) 같은 중요한 공간 오디오 단서**를 포착하여 처리합니다[^3_1][^3_2]. 이 시스템은 마이크로구조 기반 공간 센싱을 활용하여 **도달 방향(DoA, Direction of Arrival) 정보**를 추출하고, 이를 통해 음성이 어느 방향에서 오는지를 파악할 수 있습니다[^3_1][^3_2].

## **인지 심리학에서의 공간적 맥락**

시각 탐색 연구에서 공간적 맥락은 **자극들의 공간적 배열**을 의미합니다[^3_3]. 표적 자극과 방해 자극들이 화면에서 어떻게 배치되어 있는지에 대한 정보로, 사람들은 반복적인 노출을 통해 **표적의 위치와 방해 자극의 공간적 배열 사이의 연합을 학습**할 수 있습니다[^3_3]. 이는 시각적 특징 맥락과 구별되는 개념으로, 물체의 색상이나 모양 같은 시각적 특성보다는 **위치적 관계**에 초점을 맞춘 맥락입니다[^3_3].

## **공간 음향 기술에서의 맥락**

메타버스와 확장현실 환경에서 공간적 맥락은 **3차원 공간에서의 소리 재현**과 관련됩니다[^3_4]. 사용자가 가상의 공간에 실재하는 것처럼 느끼거나, 가상의 공간이 사용자의 실제 공간에 재현된 것과 같은 경험을 제공하기 위해 **공간의 위치와 방향성을 고려한 오디오 처리**가 핵심입니다[^3_4].

## **대규모 언어 모델에서의 의미**

LLM 맥락에서 공간적 맥락은 **텍스트나 언어적 단서를 넘어서는 물리적 공간 정보**를 의미합니다[^3_1]. 인간이 의미적 맥락뿐만 아니라 **소리와 위치 같은 물리적 단서를 인지하고 해석**하는 것처럼, AI 모델도 이러한 공간적 지식을 통합하여 더 전체적인 의사결정을 할 수 있도록 하는 것입니다[^3_1].

이처럼 공간적 맥락은 각 분야에서 **위치, 방향, 배열 등의 공간적 관계 정보**를 핵심으로 하되, 해당 분야의 특성에 따라 구체적인 적용 방식이 달라집니다.

<div style="text-align: center">⁂</div>

[^3_1]: https://arxiv.org/html/2504.08907v1

[^3_2]: https://arxiv.org/html/2504.08907v2

[^3_3]: https://m.riss.kr/search/detail/DetailView.do?p_mat_type=1a0202e37d52c72d\&control_no=00f6c41362ce6cb9c85d2949c297615a

[^3_4]: https://www.ppta.or.kr/webzine/2021_09/a1.html

[^3_5]: https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE11972880

[^3_6]: https://translate.google.com/translate?u=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FSpatial_contextual_awareness\&sl=en\&tl=ko\&client=srp

[^3_7]: https://translate.google.com/translate?u=https%3A%2F%2Fwww.quora.com%2FWhat-does-faster-than-real-time-mean-in-the-context-of-speech-recognition-technology\&sl=en\&tl=ko\&client=srp

[^3_8]: https://aodr.org/_PR/view/?aidx=26762\&bidx=2571

[^3_9]: http://test3.narangdesign.com/mail/jungwoo/202109/a1.html

[^3_10]: https://www.innopolis.or.kr/fileDownload?titleId=178629\&fileId=1\&fileDownType=C\&paramMenuId=MENU00999


---

# 위 논문의 임베딩 방식에 따르면 물리세계를 토큰화하고 임베딩 할 수 있다는 것인가

네, 검색 결과에 따르면 이 논문들은 **물리세계의 공간적 속성을 토큰화하고 임베딩할 수 있는 방법론**을 제시하고 있습니다.

## **공간 정보의 임베딩화**

ELSA(Spatially-Aware Language and Audio Embeddings) 모델은 **의미적 정보와 공간적 정보를 모두 포착하는 멀티모달 표현**을 학습합니다[^4_1]. 이 시스템은 오디오 인코더와 텍스트 인코더를 결합하여 **대조 학습(contrastive learning)을 통해 의미적 속성과 공간적 속성을 공동으로 학습**합니다[^4_1].

## **공간 방향의 벡터 연산**

특히 흥미로운 점은 **공간적 방향을 벡터 연산으로 조작할 수 있다**는 것입니다. ELSA의 표현 공간은 구조화되어 있어서 **두 개의 공간 기술적 텍스트 임베딩의 덧셈이나 뺄셈을 통해 공간적 소리 방향을 전치(transposition)할 수 있습니다**[^4_1].

구체적으로, 네 방향("left", "right", "front", "back")에 대한 ELSA 프로토타입을 "A sound coming from the direction [방향]"이라는 템플릿으로 획득한 후, **원래 방향의 텍스트 프로토타입을 빼고 새로운 방향의 프로토타입을 더하는 방식**으로 소리의 공간적 방향을 바꿀 수 있습니다[^4_1]. 이 방법으로 **99.7%의 정확도로 공간 방향을 성공적으로 변경**할 수 있었습니다[^4_1].

## **물리적 공간 단서의 직접적 임베딩**

SING 시스템에서는 **마이크로구조 기반 공간 센싱을 통해 방향성과 잔향 같은 물리적 공간 오디오 단서를 직접 포착**합니다[^4_2]. 이 시스템은 **모노럴 녹음 내에 공간적 단서를 직접 임베딩**하여 매우 컴팩트한 설계에서도 정확한 DoA(Direction of Arrival) 추정을 가능하게 합니다[^4_2].

## **3차원 공간 구조의 보존**

UMAP 시각화를 통해 확인된 바에 따르면, **공간 임베딩은 모든 각도에서 공간적 단서를 포착하며 방향 정보를 효과적으로 인코딩**합니다[^4_3]. 점들의 클러스터링과 각도별 조직화는 **인코더가 공간적 다양성을 표현하고 음성 신호의 근본적인 공간 구조를 보존하는 능력**을 보여줍니다[^4_3].

## **물리세계 토큰화의 의미**

이러한 접근법은 **물리세계의 공간적 속성(위치, 방향, 거리 등)을 수치적 벡터 공간으로 매핑**하여, 마치 언어의 단어를 토큰화하듯이 **물리적 공간 정보를 계산 가능한 형태로 변환**할 수 있음을 의미합니다. 특히 공간 방향을 벡터 연산으로 조작할 수 있다는 점은 **물리적 공간 관계가 수학적으로 표현되고 조작 가능한 형태로 토큰화되었음**을 보여주는 강력한 증거입니다.

따라서 이 논문들의 임베딩 방식은 **물리세계의 공간적 속성을 토큰화하고 임베딩하여 계산적으로 처리할 수 있는 프레임워크**를 제공한다고 볼 수 있습니다.

<div style="text-align: center">⁂</div>

[^4_1]: https://openreview.net/pdf?id=wDDvJzvvBR

[^4_2]: https://arxiv.org/html/2504.08907v1

[^4_3]: https://arxiv.org/html/2504.08907

[^4_4]: https://arxiv.org/html/2506.19404v1

[^4_5]: https://acta-acustica.edpsciences.org/articles/aacus/full_html/2022/01/aacus220021/aacus220021.html

[^4_6]: https://www.sciencedirect.com/science/article/abs/pii/S0141938224000726

[^4_7]: https://openreview.net/pdf/f3811b201a0d351466a3e145f039be2f068b9a6c.pdf

[^4_8]: https://openreview.net/forum?id=n61LWY45LY

[^4_9]: https://arxiv.org/html/2504.01002v2

[^4_10]: https://proceedings.neurips.cc/paper_files/paper/2024/file/ff1f4141fa2a2d5d5aca6762cfbe6b57-Paper-Conference.pdf

