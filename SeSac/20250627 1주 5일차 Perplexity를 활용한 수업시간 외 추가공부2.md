<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" class="logo" width="120"/>

# OpenAI의 Whisper 모델은 다국어 음성 인식을 위한 혁신적인 접근 방식을 사용하며, 언어 임베딩과 관련된 독특한 메커니즘을 갖추고 있습니다.

언어 식별 및 토큰 기반 시스템
Whisper는 특별한 토큰 시스템을 통해 언어를 처리합니다. 모델은 먼저 누군가 말하고 있는 언어를 감지하는 것부터 시작하며, 99개 각각의 언어마다 독특한 토큰을 사용합니다5. 이러한 언어별 토큰은 모델이 어떤 언어로 음성이 입력되었는지 식별하고, 해당 언어에 맞는 처리를 수행할 수 있도록 합니다.
멀티태스킹 아키텍처와 마커 토큰
Whisper의 언어 처리는 '마커(Markers)'라고 하는 특별한 토큰을 통해 이루어집니다. 모델이 이러한 마커 토큰을 전달받아서 어떤 작업을 수행할지 지시를 받는 구조입니다5. 이 시스템은 다음과 같이 작동합니다:
언어 감지: 99개 언어 각각에 대한 고유 토큰 사용
작업 선택: 음성을 전사할지, 번역할지 등을 결정하는 토큰
타임스탬프: 필요시 각 단어의 시작 및 종료 시간을 예측
"음성 없음" 토큰: 오디오에 음성이 없을 때 "<|nospeech|>" 토큰 예측
다국어 학습 데이터와 성능
Whisper는 680,000시간에 해당하는 레이블이 지정된 오디오 데이터로 훈련되었으며, 그 중 117,000시간(약 17%)은 非영어 오디오와 해당하는 非영어 언어의 전사본입니다5. 이러한 방대한 다국어 데이터셋을 통해 모델은 총 99개 언어를 지원할 수 있게 되었습니다5.
흥미롭게도, Whisper의 성능은 언어별 학습 데이터의 양과 강한 상관관계를 보입니다. 연구에 따르면 WER의 로그와 언어당 학습 데이터 양의 로그 사이에 0.83의 강한 제곱 상관 계수가 있으며, 학습 데이터가 16배 증가할 때마다 WER이 반으로 줄어드는 패턴을 보입니다6.
언어적 특성과 성능 차이
Whisper의 언어 처리 성능은 언어의 특성에 따라 차이를 보입니다. 고유한 스크립트를 가지고 있고 인도-유럽 언어와 관련이 먼 언어들(히브리어, 텔루구어, 중국어, 한국어 등)은 예상보다 낮은 성능을 보이는 경우가 있습니다6. 이는 언어적 거리로 인한 전송 부족, 바이트 레벨 BPE 토크나이저의 불일치, 또는 데이터 품질의 차이 때문일 수 있습니다.
Transformer 기반 아키텍처
Whisper의 기초 모델은 Transformer의 인코더-디코더 구조를 따릅니다3. 음성 데이터가 인코더에 입력되면, 해당 음성 데이터로 처리해야 하는 내용을 디코더에 입력하는 방식입니다. 인코더에 입력되는 음성 데이터는 30초 단위로 분할되며, 이후 Mel-Spectrogram을 통해 변환되어 처리됩니다3.
이러한 종합적인 언어 임베딩 시스템을 통해 Whisper는 제로샷(Zero-Shot) 환경에서도 뛰어난 다국어 음성 인식 성능을 보여주며, 추가적인 파인튜닝 없이도 전사와 번역 작업을 동시에 수행할 수 있는 강력한 모델로 자리잡았습니다45.
https://jpjp9292.tistory.com/12
https://tech.osci.kr/openai-whisper와-함께하는-음성-인식/
https://stibee.com/api/v1.0/emails/share/BcufvAuC1vo7FYjMr5OQojuR6fPQpWo
https://nlpcloud.com/ko/how-to-install-and-deploy-whisper-the-best-open-source-alternative-to-google-speech-to-text.html
https://turingpost.co.kr/p/topic-15-openai-whisper
https://kimjy99.github.io/논문리뷰/whisper/
https://devwriter.tistory.com/102
https://www.toolify.ai/ko/tool/whisper-github
https://learn.microsoft.com/ko-kr/azure/ai-services/openai/concepts/models

위에서 wer이란

Word Error Rate (WER)는 음성 인식 시스템이나 기계 번역 시스템의 성능을 평가하는 데 사용되는 핵심 지표입니다[^1_1][^1_2]. WER은 자동 음성 인식(ASR) 시스템이 생성한 전사 결과와 실제 정답(참조 전사본) 사이의 차이를 정량적으로 측정합니다[^1_3].

## WER의 정의와 계산 방법

WER은 다음 공식으로 계산됩니다[^1_1][^1_2]:

\$ WER = \frac{S + D + I}{N} \$

여기서:

- **S (Substitutions)**: 대체 오류 - 한 단어가 다른 단어로 잘못 인식된 경우
- **D (Deletions)**: 삭제 오류 - 실제로 말한 단어가 전사에서 누락된 경우
- **I (Insertions)**: 삽입 오류 - 실제로 말하지 않은 단어가 전사에 추가된 경우
- **N**: 참조 전사본의 총 단어 수


## 실제 계산 예시

예를 들어, 원본 문장이 "The quick brown fox jumps over the lazy dog" (9단어)이고, 음성 인식 시스템이 "The brown fox jumps again over the dog"로 전사했다면[^1_4]:

- 대체 오류 1개: "quick" → "brown"
- 삽입 오류 1개: "again" 추가
- 삭제 오류 1개: "lazy" 누락

따라서 WER = (1 + 1 + 1) / 9 = 0.33 또는 33%가 됩니다[^1_4].

## WER의 특징과 중요성

**WER의 범위**: WER은 일반적으로 0에서 1 사이의 값을 가지며, 0은 완벽한 정확도를, 1은 완전히 다른 결과를 의미합니다[^1_1]. 그러나 삽입 오류가 많을 경우 WER이 1을 초과할 수도 있습니다[^1_1][^1_3].

**표준화된 비교 도구**: WER은 서로 다른 ASR 시스템을 비교하거나 동일한 시스템의 성능 개선을 추적하는 표준화된 방법을 제공합니다[^1_3][^1_5]. 개발자들은 WER을 통해 시스템의 약점을 식별하고 개선 방향을 설정할 수 있습니다.

## WER의 한계점

WER은 유용한 지표이지만 몇 가지 한계가 있습니다[^1_3][^1_6]:

- **모든 오류를 동등하게 처리**: 중요한 단어의 오류와 덜 중요한 단어의 오류를 구분하지 않습니다
- **의미적 영향 무시**: 오류가 전체 의미에 미치는 영향을 고려하지 않습니다
- **맥락 정보 부족**: 오류의 원인이나 맥락적 중요성을 반영하지 않습니다


## Whisper와 WER의 관계

OpenAI의 Whisper 모델에서 언급하신 것처럼, WER은 다국어 음성 인식 시스템의 성능을 평가하는 핵심 지표로 사용됩니다. Whisper의 경우 언어별 학습 데이터의 양과 WER 사이에 강한 상관관계가 있으며, 학습 데이터가 16배 증가할 때마다 WER이 반으로 줄어드는 패턴을 보입니다. 이는 WER이 단순히 성능 측정뿐만 아니라 모델 개선의 방향성을 제시하는 중요한 도구임을 보여줍니다.

WER은 음성 인식 기술의 발전과 함께 계속해서 중요한 평가 지표로 활용되고 있으며, 특히 다국어 환경에서의 성능 비교와 개선에 핵심적인 역할을 하고 있습니다[^1_7][^1_6].

<div style="text-align: center">⁂</div>

[^1_1]: https://en.wikipedia.org/wiki/Word_error_rate

[^1_2]: https://www.rev.com/resources/what-is-wer-what-does-word-error-rate-mean

[^1_3]: https://milvus.io/ai-quick-reference/what-is-the-word-error-rate-wer-in-speech-recognition

[^1_4]: https://www.youtube.com/watch?v=hoEWRdHi7dI

[^1_5]: https://blog.kensho.com/word-error-rate-primer-a5808dcf4751?gi=fa14ea3ef928

[^1_6]: https://klu.ai/glossary/wer-score

[^1_7]: https://www.gladia.io/blog/what-is-wer

[^1_8]: https://arxiv.org/abs/2404.16743

[^1_9]: https://www.sciencedirect.com/topics/computer-science/word-error-rate

[^1_10]: https://www.isca-archive.org/interspeech_2024/gong24b_interspeech.pdf


---

# Mel-Spectrogram이란

Mel-Spectrogram은 음성 신호를 인간의 청각 특성에 맞게 변환하여 시각화한 음성 분석 도구입니다. 일반적인 스펙트로그램을 사람의 귀가 소리를 인식하는 방식에 맞춰 개선한 것으로, 딥러닝 기반 음성 처리 분야에서 핵심적인 특징 추출 방법으로 사용됩니다[^2_1][^2_3][^2_5].

## 기본 개념과 원리

Mel-Spectrogram은 **STFT(Short Time Fourier Transform)**를 기반으로 한 일반 스펙트로그램에 **Mel-scale** 변환을 적용한 결과물입니다[^2_7]. 스펙트로그램이 시간에 따른 주파수 성분의 변화를 보여주는 2차원 그래프라면, Mel-Spectrogram은 여기에 인간의 청각 특성을 반영하여 더욱 효과적인 음성 분석을 가능하게 합니다[^2_3].

## 인간 청각 특성의 반영

인간의 청각 시스템은 주파수를 선형적으로 인식하지 않습니다. 특히 **저주파수 영역에서 주파수 변화에 더 민감**하고, 고주파수로 갈수록 변화에 둔감해집니다[^2_2][^2_3]. 예를 들어, 500Hz에서 1000Hz로의 변화는 쉽게 구분할 수 있지만, 10000Hz에서 20000Hz로의 변화는 잘 인식하지 못합니다[^2_2].

이러한 특성을 반영하기 위해 **Mel-scale**이 개발되었습니다. Mel-scale은 다음 공식으로 표현됩니다[^2_9]:

\$ m = 2595 \cdot \log(1 + \frac{f}{700}) \$

여기서 m은 mel 단위, f는 Hz 단위의 주파수입니다[^2_2][^2_9].

## 생성 과정

Mel-Spectrogram의 생성 과정은 다음과 같습니다[^2_7][^2_9]:

1. **STFT 수행**: 음성 신호를 짧은 시간 구간으로 나누어 각 구간에 대해 푸리에 변환을 적용
2. **Mel Filter Bank 적용**: 주파수 스펙트럼을 Mel-scale로 변환하는 삼각형 필터들을 적용[^2_6]
3. **데시벨 변환**: 진폭을 데시벨 단위로 변환하여 인간의 소리 세기 인식 특성 반영[^2_9]

## 기술적 특징

**차원 축소 효과**: Mel-Spectrogram은 일반 스펙트로그램보다 크기가 작아서 학습 속도와 메모리 효율성 면에서 유리합니다[^2_3]. 일반적으로 80개 이상의 특징을 한 번에 추출할 수 있어 효율적인 특징 표현이 가능합니다[^2_1].

**Mel Filter Bank**: 1kHz 이하에서는 중심 주파수가 균일하게 나누어지고, 1kHz 이상에서는 로그 스케일을 따르는 특성을 가집니다[^2_7]. 이는 주파수 영역에서 지수적으로 넓어지는 삼각형 필터로 구현됩니다[^2_6].

## 활용 분야

Mel-Spectrogram은 다양한 음성 처리 분야에서 핵심적인 역할을 합니다:

- **음성 인식**: 자동 음성 인식 시스템의 입력 특징으로 활용[^2_3][^2_6]
- **음성 합성**: 딥러닝 기반 음성 생성 모델의 입력 데이터로 사용[^2_5]
- **화자 인식**: 개인의 음성 특성 분석[^2_4][^2_6]
- **감정 인식**: 음성에서 감정 상태 분석[^2_4][^2_6]
- **딥보이스 탐지**: 가짜 음성 탐지 시스템[^2_7]


## 구현과 도구

실제 구현에서는 **librosa** 라이브러리의 `melspectrogram` 함수를 주로 사용합니다[^2_4]. 이 함수는 음성 데이터를 입력받아 자동으로 Mel-Spectrogram을 생성하며, 다양한 파라미터 조정을 통해 원하는 특성의 결과를 얻을 수 있습니다[^2_4][^2_10].

Mel-Spectrogram은 단순히 기술적 도구를 넘어서 **인간 중심의 AI 개발**이라는 철학을 반영합니다. 사람이 듣는 방식대로 소리를 인식하게 함으로써, 인공지능이 사람의 문제를 더 효과적으로 해결할 수 있도록 돕는 핵심 기술입니다[^2_1].

<div style="text-align: center">⁂</div>

[^2_1]: https://dohyeon-ai.tistory.com/2

[^2_2]: https://judy-son.tistory.com/6

[^2_3]: https://ahnjg.tistory.com/93

[^2_4]: https://blog.naver.com/sooftware/221629511027

[^2_5]: https://velog.io/@tobigsvoice1516/4주차-딥러닝-기반-음성합성2

[^2_6]: https://xangmin.tistory.com/61

[^2_7]: http://journal.auric.kr/kieep/XmlViewer/f424187

[^2_8]: https://jeongwooyeol0106.tistory.com/109

[^2_9]: https://heeya-stupidbutstudying.tistory.com/entry/DL-딥러닝-음성-이해-Introduction-to-sound-data-analysis

[^2_10]: https://wikidocs.net/193588


---

# 랭체인의 역할

랭체인(LangChain)은 대형 언어 모델(LLM)을 활용한 애플리케이션 개발을 위한 오픈소스 프레임워크로, 복잡한 AI 시스템 구축을 단순화하고 표준화하는 핵심적인 역할을 수행합니다[^3_1][^3_3][^3_5].

## LLM과 외부 시스템 간의 연결 다리

랭체인의 가장 중요한 역할은 GPT-4, Claude, Gemini 같은 초거대 언어 모델과 다양한 데이터 소스를 연결하는 것입니다[^3_1]. 이는 마치 JDBC 드라이버가 백엔드 데이터베이스의 구현 세부 정보를 추상화하는 것과 유사하게, LLM의 복잡한 구현 세부사항을 간단하고 통합된 API로 제공합니다[^3_3]. 개발자들은 코드를 크게 변경하지 않고도 다양한 모델을 쉽게 교체하거나 대체할 수 있습니다[^3_3].

## 복합적 AI 애플리케이션 구축 지원

랭체인은 단순한 텍스트 생성을 넘어서 복잡하고 유용한 애플리케이션을 만들 수 있도록 설계되었습니다[^3_5]. 주요 기능들은 다음과 같습니다:

**프롬프트 엔지니어링**: LLM에 적절한 입력을 구성하는 기능을 제공합니다[^3_1].

**메모리 관리**: 대화 이력을 저장하고 컨텍스트를 유지하여 연속적인 대화가 가능하도록 합니다[^3_1].

**체인(Chains)**: 여러 개의 LLM 호출을 연결해 복잡한 작업을 순차적으로 수행할 수 있게 합니다[^3_1].

**에이전트(Agents)**: API나 도구와 연결하여 동적으로 행동하는 AI를 구현할 수 있습니다[^3_1].

## 검색 증강 생성(RAG) 구현

랭체인의 핵심 강점 중 하나는 검색 기능으로 언어 모델을 보강하는 능력입니다[^3_6]. 검색 증강 생성을 통해 언어 모델이 데이터베이스, API, 문서 저장소와 같은 외부 데이터 소스에서 관련 정보에 액세스하고 통합할 수 있습니다[^3_6]. 이를 통해 외부 문서나 데이터베이스에서 정보를 검색하여 더욱 정확하고 최신의 정보를 제공하는 애플리케이션을 구축할 수 있습니다[^3_1].

## 실시간 데이터 처리 및 반응형 시스템

랭체인은 실시간 데이터와 상호작용하는 데이터 반응형 애플리케이션을 구축할 수 있는 기능을 제공합니다[^3_5]. 사용자가 입력한 정보를 즉각적으로 처리하고 실시간으로 반응하는 시스템을 구현할 수 있어, 실시간 시장 변화에 따른 투자 전략 조정이나 맞춤형 학습 콘텐츠 제공과 같은 애플리케이션 개발이 가능합니다[^3_5].

## 기업용 AI 솔루션 개발 가속화

기업 환경에서 랭체인은 구조화된 데이터 액세스로 언어 모델의 잠재력을 활용하고, 최신 지식과 정보를 반영하며, 엔터프라이즈 AI 애플리케이션 개발을 간소화하는 역할을 합니다[^3_6]. 모듈식 아키텍처와 재사용 가능한 구성 요소를 통해 개발팀의 생산성을 향상시키고, 맞춤형 지능형 솔루션의 신속한 프로토타이핑 및 배포를 가능하게 합니다[^3_6].

## 다양한 도구와 서비스 통합

랭체인은 음성 합성을 위한 일레븐랩스 API, Google 드라이브 데이터 연결, Wolfram Alpha 지식 통합 등 다양한 외부 도구와 서비스를 쉽게 연결할 수 있는 기능을 제공합니다[^3_6]. 이를 통해 개발자들은 복잡한 통합 작업 없이도 다양한 기능을 갖춘 AI 애플리케이션을 구축할 수 있습니다.

결과적으로 랭체인은 **"똑똑한 팀원"**처럼 업무 프로세스를 정의하고 필요한 여러 툴을 연결하여 순차적으로 업무를 해결해 나가는 역할을 수행하며[^3_9], LLM의 잠재력을 극대화하여 실질적이고 복잡한 AI 애플리케이션을 쉽게 개발할 수 있도록 지원하는 핵심 플랫폼 역할을 하고 있습니다.

<div style="text-align: center">⁂</div>

[^3_1]: https://www.binarity.co.kr/post/랭체인이란-초거대-모델을-활용한-문서-업데이트-및-콜봇-구축-방법

[^3_2]: https://www.gpters.org/dev/post/langchain-agentreul-hwalyonghan-eumseong-injeung-bos-qRJn1966axWlpK7

[^3_3]: https://velog.io/@judemin/Lang-Chain-구축

[^3_4]: https://velog.io/@ongddree/책-챗GPT와-랭체인을-활용한-LLM-기반-AI-앱-개발

[^3_5]: https://www.samsungsds.com/kr/insights/what-is-langchain.html

[^3_6]: https://skimai.com/ko/랭체인-소개-주요-엔터프라이즈-사용-사례-주요-도구/

[^3_7]: https://www.hanbit.co.kr/product/C2663026630

[^3_8]: https://wikidocs.net/234094

[^3_9]: https://blog.naver.com/post6155/223818595450

[^3_10]: https://wikidocs.net/233805


---

# 인간이 아닌 인간외 동물의 의사소통을 이해하고 소통할 수 있는 llm개발을 한다고 할 떄 Mel-Spectrogram이 아닌 더 넓은 주파수의 STFT(Short Time Fourier Transform)을 사용하게 될까

동물 의사소통을 이해하는 LLM 개발에서는 Mel-Spectrogram보다 **더 넓은 주파수 범위의 STFT를 사용하는 것이 더 적절**할 것으로 예상됩니다. 이는 인간과 동물의 청각 특성 및 의사소통 방식의 근본적인 차이 때문입니다.

## 동물 의사소통의 주파수 특성

동물들은 인간과 매우 다른 주파수 범위에서 의사소통합니다. 연구 결과에 따르면 다양한 동물들이 인간의 청각 범위를 벗어난 주파수를 사용합니다[^4_1][^4_2]:

**저주파수 의사소통**: 고래, 코끼리, 카소와리(cassowary) 등은 인간이 거의 들을 수 없는 저주파수 대역에서 의사소통합니다[^4_1][^4_2]. 특히 피그미 블루 고래의 노래나 코끼리의 럼블링(rumbling) 같은 경우, 기존 STFT의 편향으로 인해 잘못 해석되는 경우가 있었습니다[^4_1].

**초음파 의사소통**: 박쥐, 돌고래 등은 20kHz 이상의 초음파 대역에서 의사소통하며, 이는 인간의 가청 주파수 범위(20Hz-20kHz)를 훨씬 초과합니다.

## Mel-Spectrogram의 한계

Mel-Spectrogram은 **인간의 청각 특성에 최적화**되어 있어 동물 의사소통 분석에는 부적절합니다:

- **인간 중심 설계**: Mel-scale은 인간이 소리를 인식하는 방식을 모방하여 설계되었으며, 1kHz 이하에서는 선형적으로, 1kHz 이상에서는 로그 스케일로 주파수를 변환합니다
- **주파수 범위 제한**: 일반적으로 인간의 음성 처리에 최적화된 주파수 범위(대략 80-8000Hz)에 집중되어 있습니다
- **정보 손실**: 동물들이 사용하는 극저주파수나 초음파 대역의 중요한 정보가 압축되거나 손실될 수 있습니다


## STFT의 장점

동물 의사소통 분석에서 STFT가 더 적합한 이유는 다음과 같습니다:

**전체 주파수 스펙트럼 보존**: STFT는 원본 신호의 모든 주파수 정보를 선형적으로 보존하여, 동물들이 사용하는 다양한 주파수 대역의 정보를 놓치지 않습니다[^4_1].

**정확한 시간-주파수 분석**: 연구에 따르면 동물 소리의 정확한 시각화를 위해서는 시간과 주파수 모두에서 정확한 구조 전달이 중요하며, 이를 위해 개선된 시간-주파수 분석 방법들이 개발되고 있습니다[^4_1][^4_2].

**유연한 파라미터 조정**: STFT는 윈도우 크기와 겹침 정도를 조정하여 다양한 동물 종의 의사소통 특성에 맞게 최적화할 수 있습니다.

## 최신 연구 동향

최근 연구에서는 동물 소리 분석을 위해 **Superlet Transform(SLT)**과 같은 새로운 방법들이 개발되고 있습니다[^4_1][^4_2]. 이 방법은 기존 STFT보다 18.48%-28.08% 더 정확한 시각화를 제공하며, 특히 저주파수 동물 소리 분석에서 우수한 성능을 보입니다[^4_1].

## 실제 적용 사례

Vision Language Model을 이용한 스펙트로그램 분류 연구에서도 다양한 환경음 분류에 일반적인 스펙트로그램이 사용되고 있으며[^4_3], 이는 동물 소리를 포함한 다양한 음향 신호 분석에서 전체 주파수 스펙트럼 정보의 중요성을 보여줍니다.

따라서 동물 의사소통을 이해하는 LLM 개발에서는 인간 중심적으로 설계된 Mel-Spectrogram보다는 **더 넓은 주파수 범위를 포괄하는 STFT나 개선된 시간-주파수 분석 방법**을 사용하는 것이 필수적일 것입니다.

<div style="text-align: center">⁂</div>

[^4_1]: https://pmc.ncbi.nlm.nih.gov/articles/PMC11220835/

[^4_2]: https://theconversation.com/our-new-tech-helps-find-hidden-details-in-whale-cassowary-and-other-barely-audible-animal-calls-234565

[^4_3]: https://arxiv.org/html/2411.12058v1

[^4_4]: https://ki-it.com/xml/40489/40489.pdf

[^4_5]: https://dsp.stackexchange.com/questions/91440/advantage-and-disadvantage-using-mel-spectrograms-over-stft-in-speech-waveform-s

[^4_6]: https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/1365-2656.13754

[^4_7]: https://pubmed.ncbi.nlm.nih.gov/18984042/

[^4_8]: https://velog.io/@pearl1058/audio-STFT-Spectrogram-Mel-Spectrogram

[^4_9]: https://github.com/heungky/trainable-STFT-Mel

[^4_10]: https://ar5iv.labs.arxiv.org/html/2307.12232

