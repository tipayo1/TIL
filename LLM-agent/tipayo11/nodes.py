# nodes.py

from dotenv import load_dotenv
load_dotenv()

import json
from typing import Dict, List, Tuple, Optional, TypedDict, Literal, cast, Any

from langchain_core.documents import Document
from langchain_core.messages import AIMessage

from state import State
from utils import get_llm, extract_query_terms_llm, compute_retrieval_metrics
from scripts.create_pinecone_index import get_vectorstore

# =============================================
# ê³µí†µ ë„ìš°ë¯¸
# =============================================
def _append_path(state: State, node: str) -> List[str]:
    path = list(state.get("execution_path") or [])
    path.append(node)
    return path

def _get_question(state: State) -> str:
    q = (state.get("user_question") or "").strip()
    if q:
        return q
    msgs = state.get("messages") or []
    for m in reversed(list(msgs)):
        content = None
        role = None
        if hasattr(m, "content"):
            content = getattr(m, "content", None)
            role = getattr(m, "type", None) or getattr(m, "role", None)
        if isinstance(m, dict):
            content = m.get("content", content)
            role = m.get("type") or m.get("role") or role
        if isinstance(content, str) and content.strip():
            if role in (None, "human", "user"):
                return content.strip()
    return ""

# =============================================
# Node: ì‚¬ìš©ì ì§ˆë¬¸ ì •ì œ
# =============================================
def refine_question(state: State) -> dict:
    _llm = get_llm("gen")
    question = _get_question(state)
    prompt = f"""
ë‹¹ì‹ ì€ "ì •ë³´í†µì‹ ê¸°íší‰ê°€ì›(IITP)" ê¸°ê´€ë‚´ë¶€ê·œì • ì±—ë´‡ì˜ ì „ì²˜ë¦¬ ë…¸ë“œì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì •ì œí•´ ì£¼ì„¸ìš”.

ê·œì¹™:
1) í•œêµ­ì–´ ìœ ì§€, ë¶ˆí•„ìš” íŠ¹ìˆ˜ë¬¸ì ì œê±°, ê³µë°± ì •ë¦¬
2) HR ìš©ì–´ í‘œì¤€í™” (ë°˜ì°¨/ëŒ€ì²´íœ´ê°€/ë³µì§€ í¬ì¸íŠ¸ ë“±)
3) í•œêµ­ì–´ ì—†ì´ ì „ë¶€ ì˜ì–´ë©´ "invalid_input"

ì‚¬ìš©ì ì§ˆë¬¸:
{question}

ìœ„ ê·œì¹™ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ ë‚´ìš©ì€ ì œê±°í•˜ê³  ì¶œë ¥í•˜ë¼.
""".strip()
    result = _llm.invoke(prompt).content.strip() if question else ""
    return {
        "user_question": question,
        "refined_question": result,
        "execution_path": _append_path(state, "refine_question"),
    }

# =============================================
# Node: RPG ì œì•ˆ(ê³„íš) í•©ì„±
# =============================================
class RPGPlan(TypedDict, total=False):
    query: str
    strategies: Dict[str, Any]  # {"retrieval":{"k":int}, "rerank":{...}, "generator":{...}}
    terms: List[str]

def compose_rpg(state: State) -> dict:
    """ì§ˆë¬¸â†’ì¿¼ë¦¬/ìš©ì–´/ì „ëµ(k) ê²½ëŸ‰ RPG ê³„íš ìƒì„±"""
    q = state.get("refined_question") or _get_question(state) or ""
    terms = extract_query_terms_llm(q)
    # ê°„ê²°í•œ ê¸°ë³¸ ì „ëµ (LLM ê°€ì´ë“œë¼ì¸ í¬í•¨)
    plan: RPGPlan = {
        "query": q,
        "terms": terms,
        "strategies": {
            "retrieval": {"k": int((len(terms) // 3) + 3)},  # 3~5 ê¸°ë³¸
            "rerank": {"mode": "llm-score", "top_k": 3},
            "generator": {"style": "concise", "cite": True},
        },
    }
    return {
        "rpg_plan": plan,
        "execution_path": _append_path(state, "compose_rpg"),
        "xp": int(state.get("xp", 0)),
    }

# =============================================
# Node: ë¦¬íŠ¸ë¦¬ë²„ (RPG ê³„íš ì ìš©)
# =============================================
def retrieve(state: State) -> dict:
    vs = get_vectorstore(index_name="IITP-rules")
    plan = state.get("rpg_plan") or {}
    refined_question = (plan.get("query") or state.get("refined_question") or _get_question(state) or "").strip()
    if not refined_question:
        return {"retrieved_docs": [], "execution_path": _append_path(state, "retrieve")}

    # këŠ” ê³„íš ìš°ì„ , í´ë°± 3
    k = int(((plan.get("strategies") or {}).get("retrieval") or {}).get("k", 3))
    # termsê°€ ìˆìœ¼ë©´ ë³´ê°• ì§ˆì˜
    terms = plan.get("terms") or []
    if terms:
        refined_question = f"{refined_question} | í‚¤ì›Œë“œ: " + ", ".join(terms)

    retriever = vs.as_retriever(search_kwargs={"k": k})
    docs = retriever.invoke(refined_question)
    return {
        "retrieved_docs": docs,
        "execution_path": _append_path(state, "retrieve"),
    }

# =============================================
# Node: ì¬ìˆœìœ„í™”(LLM ì ìˆ˜)
# =============================================
def rerank(state: State) -> dict:
    llm = get_llm("reranker")
    question = _get_question(state)
    docs_in = state.get("retrieved_docs") or []
    if not question or not docs_in:
        return {
            "retrieved_docs": docs_in,
            "execution_path": _append_path(state, "rerank"),
        }

    import re
    scored: List[Tuple[Document, float]] = []
    for doc in docs_in:
        prompt = f"""
ì§ˆë¬¸: "{question}"
ë¬¸ì„œ ë‚´ìš©: "{doc.page_content}"
0~1 ì‚¬ì´ ìˆ«ìë¡œ ê´€ë ¨ë„ë§Œ ì¶œë ¥:
""".strip()
        txt = (llm.invoke(prompt).content or "").strip()
        cleaned = txt.replace(",", ".")
        m = re.search(r"[-+]?\d*\.?\d+(?:[eE][-+]?\d+)?", cleaned)
        try:
            score = float(m.group()) if m else 0.0
        except Exception:
            score = 0.0
        score = max(0.0, min(1.0, score))
        scored.append((doc, score))
    scored.sort(key=lambda x: x[1], reverse=True)

    plan = state.get("rpg_plan") or {}
    top_k = int(((plan.get("strategies") or {}).get("rerank") or {}).get("top_k", 3))
    top_docs = [doc for doc, _ in scored[:top_k]]

    return {
        "retrieved_docs": top_docs,
        "execution_path": _append_path(state, "rerank"),
    }

# =============================================
# Node: ë©”íŠ¸ë¦­ ì‚°ì¶œ ë° XP ë¶€ì—¬
# =============================================
def award_xp(state: State) -> dict:
    plan = state.get("rpg_plan") or {}
    terms = plan.get("terms") or extract_query_terms_llm(state.get("refined_question") or "")
    docs = state.get("retrieved_docs") or []
    met = compute_retrieval_metrics(terms, docs)

    # ê³„íšì˜ k ë°˜ì˜
    k_planned = int(((plan.get("strategies") or {}).get("retrieval") or {}).get("k", len(docs) or 1))
    met["k"] = k_planned

    return {
        "retrieval_metrics": met,
        "execution_path": _append_path(state, "award_xp"),
    }

# =============================================
# Node: í™•ì¥ ê²€ìƒ‰ (LLM ìš©ì–´ í™•ì¥ + k ì¦ë¶„)
# =============================================
def expand_search(state: State) -> dict:
    plan = dict(state.get("rpg_plan") or {})
    strategies = dict((plan.get("strategies") or {}))
    retr = dict((strategies.get("retrieval") or {}))

    # k ì¦ë¶„ (ì„ í˜• í™•ì¥)
    k_curr = int(retr.get("k", 3))
    k_next = k_curr + 2
    retr["k"] = k_next

    # ìš©ì–´ í™•ì¥
    base_q = plan.get("query") or state.get("refined_question") or _get_question(state) or ""
    new_terms = extract_query_terms_llm(base_q)
    old_terms = plan.get("terms") or []
    merged_terms = list(dict.fromkeys((old_terms or []) + (new_terms or [])))[:10]

    plan["terms"] = merged_terms
    strategies["retrieval"] = retr
    plan["strategies"] = strategies

    return {
        "rpg_plan": plan,
        "xp": int(state.get("xp", 0)) + 1,
        "execution_path": _append_path(state, "expand_search"),
    }

# =========================
# Answer_type: Rag_answer
# =========================
def generate_rag_answer(state: State) -> dict:
    _llm = get_llm("gen")
    question = _get_question(state)
    if not question:
        return {
            "final_answer": "ë¬¸ì„œì— ê·¼ê±°ê°€ ì—†ì–´ ë‹µë³€ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.",
            "execution_path": _append_path(state, "generate_rag_answer"),
        }

    context = ""
    for i, doc in enumerate(state.get("retrieved_docs", []), start=1):
        context += f"[{i}] ({doc.metadata.get('source', 'unknown')})\n{doc.page_content}\n\n"
    if not context.strip():
        return {
            "final_answer": "ë¬¸ì„œì— ê·¼ê±°ê°€ ì—†ì–´ ë‹µë³€ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤. ê´€ë ¨ ì¶œì²˜ê°€ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            "execution_path": _append_path(state, "generate_rag_answer"),
        }

    prompt = f"""
ë‹¹ì‹ ì€ "ì •ë³´í†µì‹ ê¸°íší‰ê°€ì›(IITP)"ì˜ ê¸°ê´€ë‚´ë¶€ê·œì • ì•ˆë‚´ ì±—ë´‡ì…ë‹ˆë‹¤.
ì•„ë˜ ì¶œì²˜ ë¬¸ì„œ ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ì´ ì—†ìœ¼ë©´ "ë¬¸ì„œì— ê·¼ê±°ê°€ ì—†ì–´ ë‹µë³€ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•˜ì„¸ìš”.
ì¸ìš©í•œ ë¬¸ì¥ ëì—ëŠ” [ì¶œì²˜ ë²ˆí˜¸]ë¥¼ ë¶™ì´ê³ , ë§ˆì§€ë§‰ì— 'ì¶œì²˜ ëª©ë¡'ì„ ì •ë¦¬í•˜ì„¸ìš”.

# ì§ˆë¬¸
{question}

# ì¶œì²˜ ë¬¸ì„œ
{context}

# ë‹µë³€
""".strip()
    answer = _llm.invoke(prompt).content.strip()
    return {
        "messages": [AIMessage(content=answer)],
        "final_answer": answer,
        "execution_path": _append_path(state, "generate_rag_answer"),
    }

# =============================================
# Node: RAG ë‹µë³€ ê²€ì¦
# =============================================
def verify_rag_answer(state: State) -> dict:
    _llm = get_llm("judge")
    context = ""
    for doc in state.get("retrieved_docs", []):
        context += f"- {doc.page_content}\n"
    final_answer = state.get("final_answer", "")

    if not context.strip() or not final_answer.strip():
        return {"verification": "ë¶ˆì¼ì¹˜í•¨", "execution_path": _append_path(state, "verify_rag_answer")}

    prompt = f"""
ìƒì„± ë‹µë³€ì´ ë¬¸ì„œ ë‚´ìš©ì—ë§Œ ê·¼ê±°í–ˆëŠ”ì§€ ê²€ì¦í•˜ë¼.
ì™„ì „íˆ ì¼ì¹˜í•˜ë©´ 'ì¼ì¹˜í•¨', ì¡°ê¸ˆì´ë¼ë„ ë‹¤ë¥´ë©´ 'ë¶ˆì¼ì¹˜í•¨'ë§Œ ì¶œë ¥í•˜ë¼.

# ë¬¸ì„œ
{context}

# ë‹µë³€
"{final_answer}"

# íŒë‹¨ (ì¼ì¹˜í•¨/ë¶ˆì¼ì¹˜í•¨):
""".strip()
    verdict = (_llm.invoke(prompt).content or "").strip()
    final_verdict = "ì¼ì¹˜í•¨" if "ì¼ì¹˜í•¨" in verdict else "ë¶ˆì¼ì¹˜í•¨"
    return {"verification": final_verdict, "execution_path": _append_path(state, "verify_rag_answer")}

# =============================================
# Node: HR ì—¬ë¶€ íŒë³„
# =============================================
class HRAnalysis(TypedDict):
    is_hr_question: bool

DEPARTMENTS = {
    "ì¬ë¬´": {"name": "ì¬ë¬´", "email": "fi@gaida.play.com", "slack": "#ask-fi"},
    "ì´ë¬´": {"name": "ì´ë¬´", "email": "ga@gaida.play.com", "slack": "#ask-ga"},
    "ì¸í”„ë¼": {"name": "ì¸í”„ë¼", "email": "in@gaida.play.com", "slack": "#ask-in"},
    "ë³´ì•ˆ": {"name": "ë³´ì•ˆ", "email": "se@gaida.play.com", "slack": "#ask-se"},
    "ì¸ì‚¬": {"name": "ì¸ì‚¬", "email": "hr@gaida.play.com", "slack": "#ask-hr"},
}

def update_hr_status(state: State) -> State:
    """HR ì—¬ë¶€ë§Œ íŒë³„, ê·¸ ê²°ê³¼ë¥¼ ìƒíƒœì— ì €ì¥"""
    prompt = f"""
ë‹¹ì‹ ì€ "ì •ë³´í†µì‹ ê¸°íší‰ê°€ì›(IITP)"ì˜ ê¸°ê´€ë‚´ë¶€ê·œì • ì•ˆë‚´ ì±—ë´‡ì…ë‹ˆë‹¤.
ì›ë³¸/ì •ì œ ì§ˆë¬¸ì„ ë³´ê³  HR ê´€ë ¨ ì—¬ë¶€ë§Œ JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.

ì›ë³¸: "{state.get('user_question','')}"
ì •ì œ: "{state.get('refined_question','')}"

í˜•ì‹:
{{"is_hr_question": true}} ë˜ëŠ” {{"is_hr_question": false}}
""".strip()
    _llm = get_llm("router1")
    structured_llm = _llm.with_structured_output(HRAnalysis)
    result: HRAnalysis = structured_llm.invoke(prompt)
    is_hr = bool(result.get("is_hr_question", False))
    answer_type = "pending" if is_hr else "reject"
    out = {**state, "is_hr_question": is_hr, "answer_type": answer_type, "execution_path": _append_path(state, "update_hr_status")}
    return cast(State, out)

# =========================
# Answer_type: Reject
# =========================
def generate_reject_answer(state: State):
    reject_answer = "ì…ë ¥í•˜ì‹  ì§ˆë¬¸ì€ IITPì˜ ê¸°ê´€ë‚´ë¶€ê·œì • ê´€ë ¨ ë¬¸ì˜ê°€ ì•„ë‹™ë‹ˆë‹¤. IITPì˜ ê¸°ê´€ë‚´ë¶€ê·œì • ê´€ë ¨ ì§ˆë¬¸ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
    return {
        "messages": [AIMessage(content=reject_answer)],
        "final_answer": reject_answer,
        "execution_path": _append_path(state, "generate_reject_answer"),
    }

# =============================================
# Node: RAG ì—¬ë¶€ íŒë³„
# =============================================
class RAGDepartmentAnalysis(TypedDict, total=False):
    route: str            # "rag" ë˜ëŠ” "department"
    department: str       # departmentì¸ ê²½ìš°ì—ë§Œ

def _classify_rag_or_department(question: str) -> Dict[str, str]:
    """LLM í†µí•© ë¶„ë¥˜: RAG vs ë‹´ë‹¹ì + ë¶€ì„œ"""
    system_prompt = f"""
ì •ì œëœ ì§ˆë¬¸ì„ ë³´ê³  ì²˜ë¦¬ ë°©ì‹ì„ ê²°ì •í•˜ë¼.

ì •ì œëœ ì§ˆë¬¸: "{question}"

[route ê¸°ì¤€]
- rag: ë¬¸ì„œì—ì„œ ì¼ë°˜ ê·œì •ì„ ì°¾ì„ ìˆ˜ ìˆëŠ” ì •ë³´ì„± ì§ˆë¬¸
- department: ê°œì¸ ë§ì¶¤/ìŠ¹ì¸/ì‹ ê³ /ë¯¼ê° ì‚¬ì•ˆ ë“±

[ë¶€ì„œ í‚¤ì›Œë“œ]
- ì¬ë¬´: ì„¸ê¸ˆ, ì˜ˆì‚°, íšŒê³„, ì§€ì¶œ, ì†¡ê¸ˆ, ê³„ì‚°ì„œ, ì²­êµ¬ì„œ, ì§€ê¸‰, ë¹„ìš©, í™˜ê¸‰
- ì´ë¬´: ì‚¬ë¬´ì‹¤, ë¹„í’ˆ, ë¬¼í’ˆ, êµ¬ë§¤, ìˆ˜ë ¹, ìš°í¸, ì‹œì„¤, í–‰ì‚¬, ì°¨ëŸ‰, ì²­ì†Œ, ìì‚°, ì¶œì¥, ìˆ™ë°•, êµí†µ
- ì¸í”„ë¼: ì„œë²„, ë„¤íŠ¸ì›Œí¬, ì»´í“¨í„°, IT, ì†Œí”„íŠ¸ì›¨ì–´, ì¥ë¹„, ì‹œìŠ¤í…œ, ì ‘ì†, VPN, ê³„ì •, ì ‘ê·¼
- ë³´ì•ˆ: ë³´ì•ˆ, í•´í‚¹, ì •ë³´, ìœ ì¶œ, ì¹¨í•´, ëœì„¬ì›¨ì–´, ë°±ì‹ , ë°ì´í„°, ë¹„ë°€ë²ˆí˜¸, ë°©í™”ë²½, ì•…ì„±ì½”ë“œ, ì•”í˜¸
- ì¸ì‚¬: ê°œë³„ ê¸‰ì—¬/ì±„ìš©/í‰ê°€/í‡´ì§/ìƒë‹´

ì‘ë‹µ(JSON):
ragì¸ ê²½ìš°: {{"route":"rag"}}
ë‹´ë‹¹ìì¸ ê²½ìš°: {{"route":"department","department":"ë¶€ì„œëª…"}}
ë¶€ë“ì´í•˜ë©´ ì¸ì‚¬ë¡œ ì§€ì •.
""".strip()
    _llm = get_llm("router2")
    structured_llm = _llm.with_structured_output(RAGDepartmentAnalysis)
    try:
        result: RAGDepartmentAnalysis = structured_llm.invoke(system_prompt)
        return result
    except Exception:
        return {"route": "department", "department": "ì¸ì‚¬"}

def update_rag_status(state: State) -> State:
    """LLM ê¸°ë°˜ ì§ˆë¬¸ ë¶„ë¥˜ ë° ë¼ìš°íŒ… ìƒíƒœ ì—…ë°ì´íŠ¸"""
    question = state.get('refined_question') or _get_question(state) or ""
    classification_result = _classify_rag_or_department(question)
    route = classification_result.get("route")
    department_name = classification_result.get("department")

    if route == "rag":
        out = {**state, "is_rag_suitable": True, "department_info": None, "answer_type": "rag_answer"}
    else:
        department_info = DEPARTMENTS.get(department_name or "", DEPARTMENTS["ì¸ì‚¬"])
        out = {**state, "is_rag_suitable": False, "department_info": department_info, "answer_type": "department_contact"}
    out["execution_path"] = _append_path(state, "update_rag_status")
    return cast(State, out)

# =========================
# Answer_type: Department_contact
# =========================
def generate_contact_answer(state: State) -> dict:
    """ë‹´ë‹¹ì ì•ˆë‚´ ì‘ë‹µ ìƒì„±"""
    department = state.get('department_info') or {"name": "ì¸ì‚¬", "email": "hr@gaida.play.com", "slack": "#ask-hr"}
    response = f"""
í•´ë‹¹ ë¬¸ì˜ì‚¬í•­ì€ **{department['name']}íŒ€**ìœ¼ë¡œ ë¬¸ì˜í•˜ì‹œë©´ ì •í™•í•˜ê³  ë¹ ë¥¸ ë‹µë³€ì„ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ğŸ“§ ì´ë©”ì¼: {department['email']}
ğŸ’¬ ìŠ¬ë™: {department['slack']}
""".strip()
    return {
        "messages": [AIMessage(content=response)],
        "final_answer": response,
        "execution_path": _append_path(state, "generate_contact_answer"),
    }
