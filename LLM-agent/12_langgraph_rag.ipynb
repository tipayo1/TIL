{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c4bb01",
   "metadata": {},
   "source": [
    "# RAG w/ Langgraph\n",
    "`12_langgraph_rag.ipynb`\n",
    "\n",
    "- https://python.langchain.com/docs/tutorials/rag/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c66afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751b5d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# 1. Loader (웹문서)\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from bs4.filter import SoupStrainer  # pip install beautifulsoup4\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    # 문서 출처 URL\n",
    "    web_paths=('https://lilianweng.github.io/posts/2023-06-23-agent/', ),\n",
    "    # 웹페이지 안에서 필요한 정보만 선택\n",
    "    bs_kwargs={\n",
    "        'parse_only': SoupStrainer(class_=['post-content']) \n",
    "    }\n",
    "    # header_template={}\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# 2. Splitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splitted_docs = splitter.split_documents(docs)\n",
    "print(len(splitted_docs))\n",
    "\n",
    "# 3. Embedding Model\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-3-small')  # small <-> large\n",
    "\n",
    "# 4. Vectorstore (지금은 FAISS -> 클라우드-Pinecone)\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstore = FAISS.from_documents(splitted_docs, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd81a755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull('rlm/rag-prompt')\n",
    "\n",
    "for m in prompt.messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25caa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model='gpt-4.1', temperature=0)\n",
    "\n",
    "# State\n",
    "from langchain_core.documents import Document\n",
    "from typing_extensions import TypedDict, List\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Node\n",
    "# 검색 노드\n",
    "def retrieve(state: State):\n",
    "    # [ Document * 4 ]\n",
    "    retrieved_docs = vectorstore.similarity_search(state['question'], k=4)\n",
    "\n",
    "    # 나머지 return 하지 않은 state 항목들은, 알아서 그대로 감 (question, answer 는 알아서 그대로 나감)\n",
    "    return { 'context': retrieved_docs, }\n",
    "\n",
    "\n",
    "# 답변 생성노드\n",
    "def generate(state: State):\n",
    "    # Document 객체의 필요없는 정보는 다 빼고, 내용에 해당하는 page_content 만 모아서 넘기면 토큰 절약 가능.\n",
    "    context_str = ''\n",
    "    for doc in state['context']:\n",
    "        context_str += doc.page_content + '\\n------------------------\\n'\n",
    "    \n",
    "    question_with_context = prompt.invoke({'question': state['question'], 'context': context_str})\n",
    "    response = llm.invoke(question_with_context)\n",
    "    return {'answer': response.content}\n",
    "\n",
    "# Graph\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "builder = StateGraph(State)\n",
    "\n",
    "builder.add_node('retrieve', retrieve)\n",
    "builder.add_node('generate', generate)\n",
    "\n",
    "builder.add_edge(START, 'retrieve')\n",
    "builder.add_edge('retrieve', 'generate')\n",
    "builder.add_edge('generate', END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# 출력\n",
    "# from IPython.display import Image, display\n",
    "\n",
    "# display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d73867",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state = graph.invoke({'question': '에이전트 시스템에 대해 알려줘!'})\n",
    "\n",
    "final_state['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2978fa",
   "metadata": {},
   "source": [
    "*메세지 스트리밍*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1562413",
   "metadata": {},
   "outputs": [],
   "source": [
    "for message, metadata in graph.stream(\n",
    "    {\"question\": \"What is Task Decomposition? 한국어로 답해줘\"}, stream_mode=\"messages\"\n",
    "):\n",
    "    print(message.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a93494",
   "metadata": {},
   "source": [
    "## RAG +a\n",
    "- Metadata 편집\n",
    "- Query 분석 - 보완"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c70e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메타데이터 편집\n",
    "\n",
    "# 문서 63개중 1/3 지점\n",
    "third = len(splitted_docs) // 3\n",
    "\n",
    "# metadata 에 'section' 추가중 (기능적 의미는 없음.)\n",
    "for idx, doc in enumerate(splitted_docs):\n",
    "    if idx < third:\n",
    "        doc.metadata['section'] = 'beginning'\n",
    "    elif idx < third * 2:\n",
    "        doc.metadata['section'] = 'middle'\n",
    "    else:\n",
    "        doc.metadata['section'] = 'end'\n",
    "\n",
    "vectorstore = FAISS.from_documents(splitted_docs, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ffabaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State를 더 빡빡하게 정의하기 위해, 위에 따로 정의한 클래스 Search\n",
    "\n",
    "from typing import Literal  # 말그대로\n",
    "from typing_extensions import Annotated  # 할말이 좀 더 있다\n",
    "\n",
    "\n",
    "class Search(TypedDict):  # StructuredOutput 에서 사용하기 위함.\n",
    "    \"\"\"Vectorstore Search Query\"\"\"\n",
    "    # 1. 타입, 2. ... -> NOT NULL, 3. 설명(AI용)\n",
    "    query: Annotated[str, ..., 'Search query to run']\n",
    "    section: Annotated[\n",
    "        Literal['beginning', 'middle', 'end'],\n",
    "        ..., \n",
    "        'Section to query'\n",
    "    ]\n",
    "\n",
    "\n",
    "class MyState(TypedDict):\n",
    "    question: str\n",
    "    query: Search\n",
    "    context: List[Document]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1325b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node\n",
    "def analyze_query(state: MyState):\n",
    "    # Search 클래스에 맞춰 사용자 question 을 {query, section}로 바꿈\n",
    "    s_llm = llm.with_structured_output(Search)\n",
    "    query = s_llm.invoke(state['question'])\n",
    "    return {'query': query}\n",
    "\n",
    "def retrieve(state: MyState):\n",
    "    query = state['query']\n",
    "    docs = vectorstore.similarity_search(\n",
    "        query['query'],\n",
    "        # LLM이 판단한 section 과 실제 문서조각의 section이 맞을 경우에만 검색.\n",
    "        filter=lambda metadata: metadata.get('section') == query['section'],\n",
    "    )\n",
    "    return {'context': docs}\n",
    "\n",
    "def generate(state: MyState):\n",
    "    # Token 아끼기 위해, 내용만 추려서 문자열로 만들기\n",
    "    doc_str = ''\n",
    "    for doc in state['context']:\n",
    "        doc_str += doc.page_content + '\\n=====================\\n'\n",
    "    \n",
    "    question_with_context = prompt.invoke({'question': state['question'], 'context': doc_str})\n",
    "    res = llm.invoke(question_with_context)\n",
    "    return {'answer': res.content}\n",
    "\n",
    "\n",
    "builder = StateGraph(MyState)\n",
    "builder.add_node('analyze_query', analyze_query)\n",
    "builder.add_node('retrieve', retrieve)\n",
    "builder.add_node('generate', generate)\n",
    "\n",
    "builder.add_edge(START, 'analyze_query')\n",
    "builder.add_edge('analyze_query', 'retrieve')\n",
    "builder.add_edge('retrieve', 'generate')\n",
    "builder.add_edge('generate', END)\n",
    "\n",
    "graph = builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2de3a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({'question': '작업분배 뭐냐'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d187e8e9",
   "metadata": {},
   "source": [
    "## 25-08-09 대화형 RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725c466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4694dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain + Pinecone\n",
    "%pip install -q langchain-pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5f9a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from bs4.filter import SoupStrainer  # pip install beautifulsoup4\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=('https://lilianweng.github.io/posts/2023-06-23-agent/', ),\n",
    "    bs_kwargs={\n",
    "        'parse_only': SoupStrainer(class_=['post-content']) \n",
    "    }\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splitted_docs = splitter.split_documents(docs)\n",
    "\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-3-small')  # small <-> large\n",
    "\n",
    "index_name = 'gaida-1st'\n",
    "\n",
    "# 1회 실행하면, 실제 데이터가 들어가서 영구 저장 됨.\n",
    "vectorstore = PineconeVectorStore.from_documents(\n",
    "    splitted_docs, \n",
    "    index_name=index_name, \n",
    "    embedding=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c354ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존에 존재하는 index를 불러오는 코드\n",
    "index_name = 'gaida-1st'\n",
    "vectorstore = PineconeVectorStore.from_existing_index(index_name=index_name, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2710acb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool(response_format='content_and_artifact')  # 2개를 return 한다\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query\n",
    "    Args:\n",
    "        query : Query to search\n",
    "    \"\"\"\n",
    "    # 원본 Document list (artifact)\n",
    "    docs = vectorstore.similarity_search(query, k=3)\n",
    "    # 편집한 텍스트 (content)\n",
    "    result_text = '\\n\\n'.join(\n",
    "        (f'Source: {doc.metadata}\\nContent: {doc.page_content}')\n",
    "        for doc in docs\n",
    "    )\n",
    "    return result_text, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f3f09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph import START, END\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4.1', temperature=0)\n",
    "\n",
    "# Node\n",
    "def query_or_respond(state: MessagesState):\n",
    "    \"\"\"도구 호출을 하거나, 최종 응답을 한다.\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve])\n",
    "    guide = SystemMessage(\n",
    "        content=\"\"\"\n",
    "        넌 AI 어시스턴트야. 만약 사용자가 LLM이나 Agent System과 관련된 질문을 하면\n",
    "        `retrieve` Tool을 사용해야해.\n",
    "        \"\"\"\n",
    "    )\n",
    "    res = llm_with_tools.invoke([guide] + state['messages'])\n",
    "    return {'messages': [res]}\n",
    "\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\"응답 생성\"\"\"\n",
    "    tool_messages = []\n",
    "    for msg in reversed(state['messages']):  # 메세지 목록을 뒤집음: 최신 메세지부터 순회\n",
    "        if msg.type == 'tool':\n",
    "            tool_messages.append(msg)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages.reverse()\n",
    "    docs_content = '\\n\\n'.join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    # 필요 없는 Tool 메세지들을 제외하고, AI, Human, System 메시지만 모아서 정리\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    # Run\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce835d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState, StateGraph\n",
    "\n",
    "builder = StateGraph(MessagesState) # 'messages'\n",
    "\n",
    "# builder.add_node('query_or_respond', query_or_respond)  # 아래와 같은 결과\n",
    "builder.add_node(query_or_respond)\n",
    "builder.add_node(tools)\n",
    "builder.add_node(generate)\n",
    "\n",
    "# builder.set_entry_point('query_or_respond')  # 아래와 같은 말\n",
    "builder.add_edge(START, 'query_or_respond')\n",
    "builder.add_conditional_edges(\n",
    "    'query_or_respond',\n",
    "    tools_condition,\n",
    "    {END: END, 'tools': 'tools'}  # 정확하게 상황별 다음 Node 를 지정할 수 있음\n",
    ")\n",
    "builder.add_edge('tools', 'generate')\n",
    "builder.add_edge('generate', END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efba7843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c12657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 그래프를 실행하려면?\n",
    "input_state = {\n",
    "    'messages': [\n",
    "        {'role': 'user', 'content': 'Task Decomposition 이 뭐야?'}\n",
    "    ]\n",
    "}\n",
    "res = graph.invoke(input_state)\n",
    "\n",
    "for msg in res['messages']:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b84edf",
   "metadata": {},
   "source": [
    "## 대화 기록 저장하기 (Langgraph Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52973fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "# 위에 정의된 builder 사용\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bd0ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'configurable': {'thread_id': '123'}\n",
    "}\n",
    "\n",
    "input_state = {\n",
    "    'messages': [\n",
    "        {'role': 'user', 'content': 'LLM Agent는 어떤 경우에 만드는게 좋아?'}\n",
    "    ]\n",
    "}\n",
    "\n",
    "res = graph.invoke(input_state, config)\n",
    "\n",
    "for msg in res['messages']:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26905b46",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
