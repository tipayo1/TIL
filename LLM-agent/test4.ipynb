{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a99d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "    from rank_bm25 import BM25Okapi\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "\n",
    "# 1. 문서 로드 (예: 미리 크롤링해서 로컬에 저장된 텍스트 리스트)\n",
    "raw_texts = [\n",
    "    \"첫 번째 문서 내용 ...\",\n",
    "    \"두 번째 문서 내용 ...\",\n",
    "    # ... 다수의 문서\n",
    "]\n",
    "\n",
    "# 2. BM25 색인 구성\n",
    "tokenized_corpus = [doc.split() for doc in raw_texts]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# 3. FAISS 벡터스토어 준비 (이미 임베딩된 문서)\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "docs = [Document(page_content=text) for text in raw_texts]\n",
    "faiss_store = FAISS.from_documents(docs, embedding=embeddings)\n",
    "\n",
    "# 4. GPT 모델 초기화\n",
    "llm = ChatOpenAI(model=\"gpt-5-nano\", temperature=0)\n",
    "\n",
    "def hybrid_retrieve_and_generate(question: str) -> str:\n",
    "    # 1차 필터: BM25로 상위 20개 후보 선별\n",
    "    q_tokens = question.split()\n",
    "    top_n = bm25.get_top_n(q_tokens, raw_texts, n=20)\n",
    "    \n",
    "    # 2차 필터: FAISS에서 BM25 결과만 다시 검색\n",
    "    # (FAISS 검색 시, Candidate ID 리스트를 인자로 전달)\n",
    "    candidate_docs = [Document(page_content=text) for text in top_n]\n",
    "    sub_faiss = FAISS.from_documents(candidate_docs, embedding=embeddings)\n",
    "    retrieved = sub_faiss.similarity_search(question, k=5)\n",
    "    \n",
    "    # 문맥 문자열 생성\n",
    "    context = \"\\n---\\n\".join([doc.page_content for doc in retrieved])\n",
    "    \n",
    "    # RAG 프롬프트 구성\n",
    "    prompt = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following context to answer concisely (max 3 sentences).\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    )\n",
    "    \n",
    "    # GPT에 전달해 답변 생성\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "# 예시 실행\n",
    "question = \"LangGraph RAG 와 하이브리드 검색의 장점은 무엇인가?\"\n",
    "answer = hybrid_retrieve_and_generate(question)\n",
    "print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
