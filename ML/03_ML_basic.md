# ğŸ¯ TIL `03_ML_basic.md`

# Decision Tree & êµì°¨ê²€ì¦

## 1ï¸âƒ£ ë°ì´í„° ì¤€ë¹„

**ë°ì´í„°ì…‹**: wine.csv
- **íŠ¹ì§•(feature)**: alcohol, sugar, pH
- **íƒ€ê¹ƒ(target)**: class

```python
X = wine[['alcohol', 'sugar', 'pH']]
y = wine['class']

# Train / Test ë¶„ë¦¬ (80:20)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

**ì„ íƒ**: StandardScalerë¡œ ë°ì´í„° ìŠ¤ì¼€ì¼ë§
```python
ss = StandardScaler()
X_train_scaled = ss.fit_transform(X_train)
X_test_scaled = ss.transform(X_test)
```

***

## 2ï¸âƒ£ One-Hot Encoding (OHE)

### ê°œë…
- ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜
- ìˆœì„œ/í¬ê¸° ì˜ë¯¸ ì—†ì´ ê° ë²”ì£¼ë¥¼ ë…ë¦½ì ìœ¼ë¡œ í‘œí˜„

**ì˜ˆì‹œ**:
| ìƒ‰ìƒ(Color) | Label Encoding | One-Hot Encoding |
|-------------|----------------|------------------|
| Red         | 0              | 1 0 0           |
| Blue        | 1              | 0 1 0           |
| Green       | 2              | 0 0 1           |
| Red         | 0              | 1 0 0           |

```python
df_ohe = pd.get_dummies(df, columns=['Color'])
```

- âœ… **ì¥ì **: ìˆœì„œ ì™œê³¡ ë°©ì§€
- âš ï¸ **ë‹¨ì **: ë²”ì£¼ ë§ìœ¼ë©´ ì°¨ì› í­ë°œ

***

## 3ï¸âƒ£ ëª¨ë¸ í•™ìŠµ

### 3-1. Logistic Regression

```python
lr = LogisticRegression()
lr.fit(X_train_scaled, y_train)
lr.score(X_train_scaled, y_train)  # í›ˆë ¨ ì ìˆ˜
lr.score(X_test_scaled, y_test)    # í…ŒìŠ¤íŠ¸ ì ìˆ˜
```

- `.predict_proba()`: í´ë˜ìŠ¤ë³„ í™•ë¥ 
- `.coef_`, `.intercept_`: ì„ í˜• ê³„ìˆ˜ í™•ì¸

### 3-2. Decision Tree

```python
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train_scaled, y_train)
dt.score(X_train_scaled, y_train)
dt.score(X_test_scaled, y_test)
```

### íŠ¸ë¦¬ ì‹œê°í™”

```python
plt.figure(figsize=(12,10))
plot_tree(dt, max_depth=2, filled=True, feature_names=['alcohol','sugar','pH'])
plt.show()
```

### í•˜ì´í¼íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ì„¤ëª… |
|----------|------|
| `max_depth` | íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´ |
| `min_samples_split` | ë…¸ë“œë¥¼ ë¶„í• í•˜ê¸° ìœ„í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜ |
| `min_impurity_decrease` | ë…¸ë“œ ë¶„í•  ìµœì†Œ ë¶ˆìˆœë„ ê°ì†Œ |

***

## 4ï¸âƒ£ êµì°¨ê²€ì¦ (Cross Validation)

- í›ˆë ¨ ë°ì´í„°ë§Œ ì´ìš©, ëª¨ë¸ ì¼ë°˜í™” ì„±ëŠ¥ í‰ê°€
- **K-Fold ê¸°ë³¸ê°’**: 5

```python
from sklearn.model_selection import cross_validate
scores = cross_validate(dt, X_train, y_train)
np.mean(scores['test_score'])
```

**í´ë“œ ìˆ˜ ë³€ê²½ & ê³„ì¸µí™”**:
```python
from sklearn.model_selection import StratifiedKFold
splitter = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
scores = cross_validate(dt, X_train, y_train, cv=splitter)
```

âš ï¸ **Test setì€ ë§ˆì§€ë§‰ 1íšŒë§Œ ì‚¬ìš©**

***

## 5ï¸âƒ£ GridSearchCV: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

### ë‹¨ê³„
1. íƒìƒ‰í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜
2. GridSearchCV ì‹¤í–‰ (cv = êµì°¨ê²€ì¦)
3. ìµœì  ì¡°í•© í™•ì¸: `gs.best_params_`
4. ìµœì  ì¡°í•© ëª¨ë¸ë¡œ ì „ì²´ í›ˆë ¨ ë°ì´í„° í•™ìŠµ: `gs.best_estimator_`
5. ë§ˆì§€ë§‰ í…ŒìŠ¤íŠ¸ ì ìˆ˜ í™•ì¸

```python
params = {
    'min_impurity_decrease': np.arange(0.0001, 0.001, 0.0001),
    'max_depth': range(5, 20),
    'min_samples_split': range(2, 100, 10)
}

gs = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid=params,
    n_jobs=-1,
    cv=5
)

gs.fit(X_train, y_train)
print(gs.best_params_)   # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°
print(gs.best_score_)    # ìµœì  íŒŒë¼ë¯¸í„°ì˜ í‰ê·  CV ì ìˆ˜
```

### ğŸ”¹ best_score_ ê³„ì‚° ë°©ì‹

1. í›„ë³´ íŒŒë¼ë¯¸í„° ì¡°í•©ë³„ë¡œ K-Fold CV ìˆ˜í–‰
2. ê° foldì—ì„œ ê²€ì¦ ì ìˆ˜(test_score) ê³„ì‚°
3. foldë³„ ì ìˆ˜ í‰ê·  â†’ mean_test_score
4. ëª¨ë“  í›„ë³´ ì¡°í•© ì¤‘ ìµœê³  í‰ê·  ì ìˆ˜ â†’ best_score_

```python
# ë‚´ë¶€ ê³„ì‚° ê°œë…
scores_fold = []
for train_idx, val_idx in KFold.split(X_train, y_train):
    model.fit(X_train[train_idx], y_train[train_idx])
    scores_fold.append(model.score(X_train[val_idx], y_train[val_idx]))
mean_score = np.mean(scores_fold)  # í›„ë³´ íŒŒë¼ë¯¸í„° í‰ê·  ì ìˆ˜
```

âš ï¸ **Test setì€ GridSearchCV ë‚´ë¶€ ê³„ì‚°ì— ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ**

***

## 6ï¸âƒ£ ìµœì¢… ëª¨ë¸ í‰ê°€

```python
dt_best = gs.best_estimator_
dt_best.score(X_test, y_test)  # Test set 1íšŒ í‰ê°€
```

***

## 7ï¸âƒ£ ì‹œê°í™”

- **Decision Tree êµ¬ì¡° í™•ì¸**: `plot_tree(dt_best, filled=True, feature_names=[...])`
- íŠ¸ë¦¬ ê¹Šì´/ë¶ˆìˆœë„ ì œí•œ â†’ ê³¼ì í•© ë°©ì§€
- OHE í›„ ë²”ì£¼í˜• íŠ¹ì„± ì‹œê°í™” â†’ ëª¨ë¸ ì…ë ¥ í™•ì¸

***

## 8ï¸âƒ£ í•µì‹¬ í¬ì¸íŠ¸

### Train / Validation / Test
- **Validation** â†’ Cross Validationìœ¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥
- **Test** â†’ ìµœì¢… í‰ê°€ìš©, 1íšŒë§Œ

### Decision Tree ê³¼ì í•© ì£¼ì˜
- `max_depth`, `min_samples_split`, `min_impurity_decrease` ì¡°ì ˆ

### GridSearchCV
- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ + êµì°¨ê²€ì¦ ë™ì‹œ ìˆ˜í–‰
- **best_score_** = ë‚´ë¶€ CVì—ì„œ ìµœì  íŒŒë¼ë¯¸í„° í‰ê·  ê²€ì¦ ì ìˆ˜

### One-Hot Encoding
- ìˆœì„œ ì™œê³¡ ì—†ì´ ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë³€í™˜

### ì‹œê°í™”
- íŠ¸ë¦¬ êµ¬ì¡° ë° íŠ¹ì§• ì¤‘ìš”ë„ ì´í•´ì— ë„ì›€
