# 🎯 TIL `01_ML_basic.md`

# 머신러닝 기초: 지도학습, KNN, 회귀, 규제

## 1️⃣ 지도학습 vs 비지도학습

### 구분과 특징

| 구분 | 특징 | 예시 |
|------|------|------|
| **지도학습 (Supervised)** | 입력(X) + 정답(y) → 모델 학습 | 분류: 도미/빙어, 회귀: 물고기 무게 |
| **비지도학습 (Unsupervised)** | 정답 없음 → 패턴/구조 학습 | 군집(K-means), 차원축소(PCA), 이상치 탐지 |

### 포인트
- 지도학습은 **분류(Classification)**와 회귀(Regression) 두 가지
- 테스트 데이터로 모델 일반화 성능 확인 필수

***

## 2️⃣ K-Nearest Neighbors (KNN)

### 작동 방식
- **분류(Classification)**: 다수결 → 가장 많은 클래스 선택
- **회귀(Regression)**: 이웃 값 평균 → 예측값 생성

### 하이퍼파라미터 K

| K 값 | 결과 |
|------|------|
| **작음** | 훈련 데이터 노이즈에 민감 → 과대적합 |
| **큼** | 주변 데이터까지 포함 → 과소적합 |

### 주의점
- 거리 기반 모델 → 특성 단위가 다르면 스케일링 필요

***

## 3️⃣ 회귀 (Regression)

### (1) 선형 회귀 (Linear Regression)
- 직선으로 데이터 설명 → 식: `y = ax + b`
- 단순 → 과소적합 가능

### (2) 다항 회귀 (Polynomial Regression)
- 입력 특성을 제곱, 세제곱 등 확장 후 선형 회귀 적용
- 고차 다항식 → 과대적합 위험 ↑ → 규제 필요
- **Tip**: 훈련 기준으로 fit → 테스트 데이터 transform → 데이터 누수 방지

***

## 4️⃣ 규제 (Regularization)

### (1) 릿지 회귀 (Ridge, L2)
- 계수를 제곱한 값을 패널티 → 큰 계수 억제
- α ↑ → 규제 강해짐 → 훈련 점수 ↓, 일반화 점수 ↑

### (2) 라쏘 회귀 (Lasso, L1)
- 계수 절댓값 패널티 → 일부 계수 0 → 변수 선택 효과
- α ↑ → 과소적합 가능

### 공통사항
- 훈련/테스트 데이터 모두 동일 스케일링 필수 (StandardScaler)
- 규제 적용 → 과대적합 방지, 일반화 성능 향상

***

## 5️⃣ 실습 포인트

- **KNN**: 거리 기반 → 특성 스케일 중요, K 값 조절
- **선형/다항 회귀**: 차수 조절, 규제 필요
- **릿지/라쏘**: α 값 조절 → 모델 복잡도 & 과적합 균형
- **훈련/테스트 분리** 필수 → 과적합/과소적합 확인
- **다항 회귀 + 규제** → 과대적합 억제 가능

***

## 6️⃣ 오늘 학습 흐름

1. **KNN (분류/회귀)** → 데이터 시각화, K 값 조절, 스케일링
2. **선형 회귀** → 단순 모델 → 과소적합 확인
3. **다항 회귀** → 차수 확장 → 과대적합 가능성 확인
4. **규제 회귀 (Ridge/Lasso)** → α 값 조절 → 과대적합 방지

### 결론
**단순 모델 → 복잡 모델 → 규제 적용**
훈련 점수와 테스트 점수 비교 → 모델 일반화 능력 확인